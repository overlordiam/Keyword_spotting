{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7gTUwlkI6XY"
      },
      "source": [
        "# KEYWORD SPOTTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StoB0SEA7QlJ"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4330IYHiSde"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain.git\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOwJoXrXiUG7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qGXW76iV_u"
      },
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjr5k2qiX4j",
        "outputId": "f9522d8f-71a3-44c3-f42a-53e6a7bb8a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prepare_GSC.py\n"
          ]
        }
      ],
      "source": [
        "%%file prepare_GSC.py\n",
        "\n",
        "\"\"\"\n",
        "Data preparation for Google Speech Commands v0.02.\n",
        "\n",
        "Download: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "\n",
        "Author\n",
        "------\n",
        "David Raby-Pepin 2021\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from os import walk\n",
        "import glob\n",
        "import shutil\n",
        "import logging\n",
        "import torch\n",
        "import re\n",
        "import hashlib\n",
        "import copy\n",
        "import numpy as np\n",
        "from speechbrain.utils.data_utils import download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    err_msg = (\n",
        "        \"The optional dependency pandas must be installed to run this recipe.\\n\"\n",
        "    )\n",
        "    err_msg += \"Install using `pip install pandas`.\\n\"\n",
        "    raise ImportError(err_msg)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "GSC_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "\n",
        "# List of all the words (i.e. classes) within the GSC v2 dataset\n",
        "all_words = [\n",
        "    \"yes\",\n",
        "    \"no\",\n",
        "    \"up\",\n",
        "    \"down\",\n",
        "    \"left\",\n",
        "    \"right\",\n",
        "    \"on\",\n",
        "    \"off\",\n",
        "    \"stop\",\n",
        "    \"go\",\n",
        "    \"zero\",\n",
        "    \"one\",\n",
        "    \"two\",\n",
        "    \"three\",\n",
        "    \"four\",\n",
        "    \"five\",\n",
        "    \"six\",\n",
        "    \"seven\",\n",
        "    \"eight\",\n",
        "    \"nine\",\n",
        "    \"bed\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "    \"happy\",\n",
        "    \"house\",\n",
        "    \"marvin\",\n",
        "    \"sheila\",\n",
        "    \"tree\",\n",
        "    \"wow\",\n",
        "    \"backward\",\n",
        "    \"forward\",\n",
        "    \"follow\",\n",
        "    \"learn\",\n",
        "    \"visual\",\n",
        "]\n",
        "\n",
        "\n",
        "def prepare_GSC(\n",
        "    data_folder,\n",
        "    save_folder,\n",
        "    validation_percentage=10,\n",
        "    testing_percentage=10,\n",
        "    percentage_unknown=10,\n",
        "    percentage_silence=10,\n",
        "    words_wanted=[\n",
        "        \"yes\",\n",
        "        \"no\",\n",
        "        \"up\",\n",
        "        \"down\",\n",
        "        \"left\",\n",
        "        \"right\",\n",
        "        \"on\",\n",
        "        \"off\",\n",
        "        \"stop\",\n",
        "        \"go\",\n",
        "    ],\n",
        "    skip_prep=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the Google Speech Commands V2 dataset.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        path to dataset. If not present, it will be downloaded here.\n",
        "    save_folder: str\n",
        "        folder where to store the data manifest files.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "    percentage_unknown: int.\n",
        "        How much data outside of the known (i.e wanted) words to preserve; relative to the total number of known words.\n",
        "    percentage_silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "    words_wanted: list\n",
        "        The list of commands to use from the dataset.\n",
        "    skip_prep: bool\n",
        "        If True, skip data preparation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_folder = '/path/to/GSC'\n",
        "    >>> prepare_GSC(data_folder)\n",
        "    \"\"\"\n",
        "\n",
        "    if skip_prep:\n",
        "        return\n",
        "\n",
        "    # If the data folders do not exist, we need to extract the data\n",
        "    if not os.path.isdir(os.path.join(data_folder, \"train-synth\")):\n",
        "        # Check for zip file and download if it doesn't exist\n",
        "        tar_location = os.path.join(data_folder, \"speech_commands_v0.02.tar.gz\")\n",
        "        if not os.path.exists(tar_location):\n",
        "            download_file(GSC_URL, tar_location, unpack=True)\n",
        "        else:\n",
        "            logger.info(\"Extracting speech_commands_v0.02.tar.gz...\")\n",
        "            shutil.unpack_archive(tar_location, data_folder)\n",
        "\n",
        "    # Define the words that we do not want to identify\n",
        "    unknown_words = list(np.setdiff1d(all_words, words_wanted))\n",
        "\n",
        "    # All metadata fields to appear within our dataset annotation files (i.e. train.csv, valid.csv, test.cvs)\n",
        "    fields = {\n",
        "        \"ID\": [],\n",
        "        \"duration\": [],\n",
        "        \"start\": [],\n",
        "        \"stop\": [],\n",
        "        \"wav\": [],\n",
        "        \"spk_id\": [],\n",
        "        \"command\": [],\n",
        "        \"transcript\": [],\n",
        "    }\n",
        "\n",
        "    splits = {\n",
        "        \"train\": copy.deepcopy(fields),\n",
        "        \"valid\": copy.deepcopy(fields),\n",
        "        \"test\": copy.deepcopy(fields),\n",
        "    }\n",
        "\n",
        "    num_known_samples_per_split = {\"train\": 0, \"valid\": 0, \"test\": 0}\n",
        "    words_wanted_parsed = False\n",
        "    commands = words_wanted + unknown_words\n",
        "    for i, command in enumerate(commands):\n",
        "        # logger.info(\"Preparing {}/{} commands...\".format(i, len(commands)))\n",
        "\n",
        "        # Indicate once all wanted words are parsed\n",
        "        if i >= len(words_wanted) and not words_wanted_parsed:\n",
        "            num_known_samples_total = np.sum(\n",
        "                list(num_known_samples_per_split.values())\n",
        "            )\n",
        "            num_unknown_samples_total = 105829 - num_known_samples_total\n",
        "            percentage_applied_to_unknown_samples = (\n",
        "                percentage_unknown * num_known_samples_total\n",
        "            ) / num_unknown_samples_total\n",
        "            words_wanted_parsed = True\n",
        "\n",
        "        # Read all files under a specific class (i.e. command)\n",
        "        files = []\n",
        "        for dirpath, dirnames, filenames in walk(\n",
        "            os.path.join(data_folder, command)\n",
        "        ):\n",
        "            files.extend(filenames)\n",
        "            break\n",
        "\n",
        "        # Fill in all fields with metadata for each audio sample file under a specific class\n",
        "        for filename in files:\n",
        "            # Once all wanted words are parsed, only retain the required percentage of unknown words\n",
        "            if (\n",
        "                words_wanted_parsed\n",
        "                and torch.rand(1)[0].tolist()\n",
        "                > percentage_applied_to_unknown_samples / 100\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            # select the required split (i.e. set) for the sample\n",
        "            split = which_set(\n",
        "                filename, validation_percentage, testing_percentage\n",
        "            )\n",
        "\n",
        "            splits[split][\"ID\"].append(\n",
        "                command + \"/\" + re.sub(r\".wav\", \"\", filename)\n",
        "            )\n",
        "\n",
        "            # We know that all recordings are 1 second long (i.e.16000 frames). No need to compute the duration.\n",
        "            splits[split][\"duration\"].append(1.0)\n",
        "            splits[split][\"start\"].append(0)\n",
        "            splits[split][\"stop\"].append(16000)\n",
        "\n",
        "            splits[split][\"wav\"].append(\n",
        "                os.path.join(data_folder, command, filename)\n",
        "            )\n",
        "\n",
        "            splits[split][\"spk_id\"].append(re.sub(r\"_.*\", \"\", filename))\n",
        "\n",
        "            if command in words_wanted:\n",
        "                splits[split][\"command\"].append(command)\n",
        "\n",
        "                num_known_samples_per_split[split] += 1\n",
        "            else:\n",
        "                splits[split][\"command\"].append(\"unknown\")\n",
        "\n",
        "            splits[split][\"transcript\"].append(command)\n",
        "\n",
        "    if percentage_silence > 0:\n",
        "        generate_silence_data(\n",
        "            num_known_samples_per_split,\n",
        "            splits,\n",
        "            data_folder,\n",
        "            percentage_silence=percentage_silence,\n",
        "        )\n",
        "\n",
        "    for split in splits:\n",
        "        new_filename = os.path.join(save_folder, split) + \".csv\"\n",
        "        new_df = pd.DataFrame(splits[split])\n",
        "        new_df.to_csv(new_filename, index=False)\n",
        "\n",
        "\n",
        "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
        "\n",
        "\n",
        "def which_set(filename, validation_percentage, testing_percentage):\n",
        "    \"\"\"Determines which data partition the file should belong to.\n",
        "\n",
        "    We want to keep files in the same training, validation, or testing sets even\n",
        "    if new ones are added over time. This makes it less likely that testing\n",
        "    samples will accidentally be reused in training when long runs are restarted\n",
        "    for example. To keep this stability, a hash of the filename is taken and used\n",
        "    to determine which set it should belong to. This determination only depends on\n",
        "    the name and the set proportions, so it won't change as other files are added.\n",
        "\n",
        "    It's also useful to associate particular files as related (for example words\n",
        "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
        "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
        "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    filename: path\n",
        "        File path of the data sample.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result: str\n",
        "        one of 'training', 'validation', or 'testing'.\n",
        "    \"\"\"\n",
        "    base_name = os.path.basename(filename)\n",
        "    # We want to ignore anything after '_nohash_' in the file name when\n",
        "    # deciding which set to put a wav in, so the data set creator has a way of\n",
        "    # grouping wavs that are close variations of each other.\n",
        "    hash_name = re.sub(r\"_nohash_.*$\", \"\", base_name).encode(\"utf-8\")\n",
        "    # This looks a bit magical, but we need to decide whether this file should\n",
        "    # go into the training, testing, or validation sets, and we want to keep\n",
        "    # existing files in the same set even if more files are subsequently\n",
        "    # added.\n",
        "    # To do that, we need a stable way of deciding based on just the file name\n",
        "    # itself, so we do a hash of that and then use that to generate a\n",
        "    # probability value that we use to assign it.\n",
        "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
        "    percentage_hash = (\n",
        "        int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)\n",
        "    ) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
        "    if percentage_hash < validation_percentage:\n",
        "        result = \"valid\"\n",
        "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
        "        result = \"test\"\n",
        "    else:\n",
        "        result = \"train\"\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_silence_data(\n",
        "    num_known_samples_per_split, splits, data_folder, percentage_silence=26\n",
        "):\n",
        "    \"\"\"Generates silence samples.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    num_known_samples_per_split: int\n",
        "        Total number of samples of known words for each split (i.e. set).\n",
        "    splits: str\n",
        "        Training, validation and test sets.\n",
        "    data_folder: str\n",
        "        path to dataset.\n",
        "    percentage_silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "    \"\"\"\n",
        "    for split in splits:\n",
        "        num_silence_samples = int(\n",
        "            (percentage_silence / 100.0) * num_known_samples_per_split[split]\n",
        "        )\n",
        "\n",
        "        # Fetch all background noise wav files used to generate silence samples\n",
        "        search_path = os.path.join(data_folder, \"_background_noise_\", \"*.wav\")\n",
        "        silence_paths = []\n",
        "        for wav_path in glob.glob(search_path):\n",
        "            silence_paths.append(wav_path)\n",
        "\n",
        "        # Generate random silence samples\n",
        "        # Assumes that the pytorch seed has been defined in the HyperPyYaml file\n",
        "        num_silence_samples_per_path = int(\n",
        "            num_silence_samples / len(silence_paths)\n",
        "        )\n",
        "        for silence_path in silence_paths:\n",
        "            signal = read_audio(silence_path)\n",
        "            random_starts = (\n",
        "                (\n",
        "                    torch.rand(num_silence_samples_per_path)\n",
        "                    * (signal.shape[0] - 16001)\n",
        "                )\n",
        "                .type(torch.int)\n",
        "                .tolist()\n",
        "            )\n",
        "\n",
        "            for i, random_start in enumerate(random_starts):\n",
        "                splits[split][\"ID\"].append(\n",
        "                    re.sub(\n",
        "                        r\".wav\",\n",
        "                        \"/\" + str(random_start) + \"_\" + str(i),\n",
        "                        re.sub(r\".+?(?=_background_noise_)\", \"\", silence_path),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                splits[split][\"duration\"].append(1.0)\n",
        "                splits[split][\"start\"].append(random_start)\n",
        "                splits[split][\"stop\"].append(random_start + 16000)\n",
        "                splits[split][\"wav\"].append(silence_path)\n",
        "                splits[split][\"spk_id\"].append(None)\n",
        "                splits[split][\"command\"].append(\"silence\")\n",
        "                splits[split][\"transcript\"].append(None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQR6w3R3I6Xh"
      },
      "source": [
        "# Code to generate plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRwAteYfI6Xh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(train_loss, valid_loss, error_rate):\n",
        "    \"\"\"\n",
        "    Plots two graphs side by side: one for train loss and validation loss vs epochs, and the other for error rate vs epochs.\n",
        "\n",
        "    Parameters:\n",
        "    train_loss (list: floats): The training loss values for each epoch.\n",
        "    valid_loss (list: floats): The validation loss values for each epoch.\n",
        "    error_rate (list: floats): The error rate values for each epoch.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    epochs = len(train_loss)\n",
        "\n",
        "    # Convert error rate to percentage\n",
        "    error_rate = [v * 100 for v in error_rate]\n",
        "\n",
        "    # Plotting both graphs side by side\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Plotting the first graph (train loss vs validation loss)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, marker='o', label='Train Loss')\n",
        "    plt.plot(epochs, valid_loss, marker='o', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train Loss vs Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting the second graph (error rate)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, error_rate, marker='o', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error Rate (%)')\n",
        "    plt.title('Error Rate')\n",
        "\n",
        "    # Adjust layout to prevent overlap\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# plot([0.1,0.2,0.4], [0.2,0.3,0.5], [0.11, 0.432, 0.10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-UHZl2zv-jS"
      },
      "source": [
        "# CONTINUOUS FEATURES: FBANKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn3u38qxI6Xi"
      },
      "source": [
        "**Continuous features are real-valued features extracted from audio signals. Since they are continuous, they have fine-grained values/information of the downstream task and perform incredibly well, even with just a simple classifier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A151kfQZI6Xj"
      },
      "source": [
        "**Why Fbanks?\n",
        "There are many reasons why Fbanks is used in Speech Recognition. They are simple and intuitive. They are robust enough to be used till this day. They downsample the audio data greatly for efficient processing. Most importantly, they mimic the humans ear's perception of audio, targetting the low frequencies and maximising germane information. Hence, it is a good feature extractor for keyword spotting.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEQMFPbuwDnm"
      },
      "source": [
        "### hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjrKgdytwQl2",
        "outputId": "cec31943-7778-42c1-e435-9261f9c7c5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_xvector_fbanks.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_xvector_fbanks.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/xvect_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.001\n",
        "lr_final: 0.0001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 40 #27\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hRuLLIuwDk8"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sZHKKw9wUGD",
        "outputId": "4820c0ff-eeb3-4990-8fef-0f0e5d10d24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_fbanks.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_fbanks.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are optionally applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        if isinstance(\n",
        "            self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        ):\n",
        "            # if leaf, first normalize the wavs before feeding them to leaf\n",
        "            # no normalization is needed after LEAF\n",
        "            feats = self.modules.mean_var_norm(wavs, lens)\n",
        "            feats = self.modules.compute_features(feats)\n",
        "        else:\n",
        "            # Feature extraction and normalization\n",
        "            feats = self.modules.compute_features(wavs)\n",
        "            feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        embeddings = self.modules.embedding_model(feats)\n",
        "        outputs = self.modules.classifier(embeddings)\n",
        "\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command, lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An11YPqfwLgu"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQSeAQjmwZkk",
        "outputId": "60a2a21f-c540-4d92-abc0-0a387cd44761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/xvect_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "Downloading https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1 to /path/to/GSC/noise/data.zip\n",
            "noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1: 569MB [00:05, 108MB/s]         \n",
            "Extracting /path/to/GSC/noise/data.zip to /path/to/GSC/noise\n",
            "Downloading https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1 to /path/to/GSC/rir/data.zip\n",
            "RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1: 246MB [00:04, 51.6MB/s]          \n",
            "Extracting /path/to/GSC/rir/data.zip to /path/to/GSC/rir\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 4.5M\n",
            "* Total Number of Parameters: 4.5M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/xvect_v12/1986/save/CKPT+2024-04-26+14-06-47+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|█████████████████████| 1154/1154 [12:59<00:00,  1.48it/s, train_loss=0.122]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.12it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 0.00048\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 5.26e-04 - train loss: 1.22e-01 - valid loss: 9.37e-02, valid ErrorRate: 2.68e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-32-10+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+13-52-37+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-06-47+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|█████████████████████| 1154/1154 [12:34<00:00,  1.53it/s, train_loss=0.176]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.07it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00048 to 0.00043\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 4.79e-04 - train loss: 1.76e-01 - valid loss: 1.13e-01, valid ErrorRate: 3.15e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-44-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|█████████████████████| 1154/1154 [12:43<00:00,  1.51it/s, train_loss=0.155]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.22it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00043 to 0.00038\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 4.32e-04 - train loss: 1.55e-01 - valid loss: 1.19e-01, valid ErrorRate: 3.60e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-57-54+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-44-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|█████████████████████| 1154/1154 [12:45<00:00,  1.51it/s, train_loss=0.141]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:11<00:00, 11.91it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00038 to 0.00034\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 3.84e-04 - train loss: 1.41e-01 - valid loss: 1.16e-01, valid ErrorRate: 3.42e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-10-52+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-57-54+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|█████████████████████| 1154/1154 [12:36<00:00,  1.52it/s, train_loss=0.127]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:11<00:00, 12.10it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00034 to 0.00029\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 3.37e-04 - train loss: 1.27e-01 - valid loss: 9.91e-02, valid ErrorRate: 2.86e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-23-41+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-10-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|██████████████████████| 1154/1154 [12:36<00:00,  1.53it/s, train_loss=0.12]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:11<00:00, 11.60it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00029 to 0.00024\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 2.89e-04 - train loss: 1.20e-01 - valid loss: 9.59e-02, valid ErrorRate: 2.68e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-36-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-23-41+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+14-32-10+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|█████████████████████| 1154/1154 [12:38<00:00,  1.52it/s, train_loss=0.108]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.33it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00024 to 0.00019\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 2.42e-04 - train loss: 1.08e-01 - valid loss: 9.36e-02, valid ErrorRate: 2.59e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-49-20+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+15-36-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|████████████████████| 1154/1154 [12:39<00:00,  1.52it/s, train_loss=0.0979]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.24it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00019 to 0.00015\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 1.95e-04 - train loss: 9.79e-02 - valid loss: 9.44e-02, valid ErrorRate: 2.79e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+16-02-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|████████████████████| 1154/1154 [12:39<00:00,  1.52it/s, train_loss=0.0889]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.28it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00015 to 0.0001\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 1.47e-04 - train loss: 8.89e-02 - valid loss: 9.54e-02, valid ErrorRate: 2.68e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+16-15-04+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+16-02-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|████████████████████| 1154/1154 [12:37<00:00,  1.52it/s, train_loss=0.0811]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:12<00:00, 11.55it/s]\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 1.00e-04 - train loss: 8.11e-02 - valid loss: 9.35e-02, valid ErrorRate: 2.70e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+16-27-54+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/xvect_v12/1986/save/CKPT+2024-04-26+16-15-04+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/xvect_v12/1986/save/CKPT+2024-04-26+15-49-20+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:13<00:00, 11.43it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 17 - test loss: 6.71e-02, test ErrorRate: 2.11e-02\n"
          ]
        }
      ],
      "source": [
        "!python train_fbanks.py hparams_xvector_fbanks.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKvkGlc1I6Xv"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg26Wl0mI6Xv"
      },
      "source": [
        "**Fbanks does very good job in extracting and downsizing features from the audio signal. It does not overfit or underfit on the data as well. This only strengthens the case for continuous features in speech related tasks. The error rate dips to very low values. The final test error rate is 2%, which is amazing. They soon reach saturation near the end of 20 epochs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzxJcouPI6Xw"
      },
      "source": [
        "![image.png](attachment:4b1a1d4f-d152-4f37-baa6-bde4a0ec59fb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tezOEXKwv1W0"
      },
      "source": [
        "# CONTINUOUS SELF-SUPERVISED FEATURES: WAV2VEC, HuBERT AND WAVLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk062xrnw-04"
      },
      "source": [
        "## WAV2VEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j638XD0e0Fab"
      },
      "source": [
        "**What is Wav2Vec and why use it?\n",
        "Wav2Vec is a self-supervised framework for speech recognition. It performs exceedingly well on speech tasks especially keyword spotting. Firslty, all patterns are learned and require no handcrafted variables like filterbanks, making it robust. Moreover, they are pre-trained on large amounts of unlabelled audio data aiding them in generalization and superior performance in downstream tasks.**\n",
        "\n",
        "\n",
        "**Fine-tuning the Conv layers has little to no effect. However, fine-tuning the remaining layers of Wav2Vec improves the performance of the system.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcz5P7J46d6f"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ufzPP9K6fYW",
        "outputId": "4c862a2d-205d-48d3-ae0e-bcffb548f33d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_wav2vec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_wav2vec.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        outputs = self.modules.ssl_model(wavs, lens)\n",
        "\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "    # print(f\"train_data type: {type(train_data)}\")\n",
        "    # print(f\"dataset[0]: {train_data[0]}\")\n",
        "    # print(f\"len of dataset: {len(train_data)}\")\n",
        "    # print(train_data[:5])\n",
        "    # print(f\"type of label_enc: {type(label_encoder)}\")\n",
        "    # print(f\"dataset[0]: {label_encoder[0]}\")\n",
        "    # print(f\"len of dataset: {len(label_encoder)}\")\n",
        "    # print(label_encoder[:5])\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZpd6NAGxFT3"
      },
      "source": [
        "### hparams - freeze all the layers of Wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwQWpI-d6O-Q",
        "outputId": "9cd1fe58-284b-4380-fe84-e8ab7308d0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing hparams_wav2vec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wav2vec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wav2vec_3_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55XKF6YgxFD5"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFZwV7cy6Ujr"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/results/wav2vec_3_v12/1986/\n",
        "\n",
        "!python train_wav2vec.py hparams_wav2vec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usOJaorI6X2"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIW3Fqk9I6X2"
      },
      "source": [
        "![image.png](attachment:99fb5e74-5235-4666-8863-fadf2c3d4576.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOP9gwK-I6X2"
      },
      "source": [
        "**The model does not overfit and generalizes well. The error rate also gradually decreases with the epoch count. However, it soon starts to pleateau around the 20th epoch. The final error rate on the test set is 16.1%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ycp-xt6jph"
      },
      "source": [
        "### hparams - fine-tune all the layers of Wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjAHhbKk7Zr3",
        "outputId": "030ac4a1-d955-4534-b75c-78ef2651e6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_wav2vec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wav2vec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wav2vec_4_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: False\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsNu1xBA6jki"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzwnZ83I7Ued",
        "outputId": "44ed2156-fcec-475b-8b1a-4fe0695ad5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wav2vec_4_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "Downloading https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1 to /path/to/GSC/noise/data.zip\n",
            "noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1: 569MB [00:13, 42.5MB/s]        \n",
            "Extracting /path/to/GSC/noise/data.zip to /path/to/GSC/noise\n",
            "Downloading https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1 to /path/to/GSC/rir/data.zip\n",
            "RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1: 246MB [00:02, 96.8MB/s]          \n",
            "Extracting /path/to/GSC/rir/data.zip to /path/to/GSC/rir\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 94.4M\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+00-40-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|███████████████████| 1155/1155 [03:37<00:00,  5.30it/s, train_loss=0.00919]\n",
            "100%|█████████████████████████████████████████| 140/140 [00:09<00:00, 14.61it/s]\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 9.19e-03 - valid loss: 1.23e-01, valid ErrorRate: 1.84e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+00-58-57+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+00-40-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|███████████████████| 1155/1155 [03:35<00:00,  5.36it/s, train_loss=0.00641]\n",
            "100%|█████████████████████████████████████████| 140/140 [00:09<00:00, 14.19it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-06 to 4.8e-06\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 6.41e-03 - valid loss: 1.44e-01, valid ErrorRate: 1.97e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-02-44+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+00-58-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|███████████████████| 1155/1155 [03:35<00:00,  5.36it/s, train_loss=0.00537]\n",
            "100%|█████████████████████████████████████████| 140/140 [00:09<00:00, 14.57it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-05 to 4.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-06 to 4.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 4.78e-05, ssl_lr: 4.78e-06 - train loss: 5.37e-03 - valid loss: 1.52e-01, valid ErrorRate: 2.11e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-06-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-02-44+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|███████████████████| 1155/1155 [03:36<00:00,  5.35it/s, train_loss=0.00528]\n",
            "100%|█████████████████████████████████████████| 140/140 [00:09<00:00, 14.89it/s]\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 4.30e-05, ssl_lr: 4.30e-06 - train loss: 5.28e-03 - valid loss: 1.31e-01, valid ErrorRate: 1.79e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-10-17+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+00-32-45+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-06-30+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|███████████████████| 1155/1155 [03:36<00:00,  5.34it/s, train_loss=0.00422]\n",
            "100%|█████████████████████████████████████████| 140/140 [00:09<00:00, 14.26it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-05 to 3.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-06 to 3.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 4.30e-05, ssl_lr: 4.30e-06 - train loss: 4.22e-03 - valid loss: 1.37e-01, valid ErrorRate: 1.84e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-14-04+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec_4_v12/1986/save/CKPT+2024-04-26+01-10-17+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:11<00:00, 13.35it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 19 - test loss: 1.19e-01, test ErrorRate: 1.65e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/wav2vec_4_v12/1986/\n",
        "\n",
        "!python train_wav2vec.py hparams_wav2vec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlWxDGqSI6X8"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFZ5neCII6X9"
      },
      "source": [
        "**The model seems to overfit slightly on the data even though we just use a simple linear layer as a classifier. This is because we are fine-tuning all the layers of Wav2Vec on the downstream task on a relatively small dataset. With more data, this problem can be tackled. Ideally, the training would have been stopped at epoch 5 or 6 but for clarity and understanding, we continue to train for 20 epochs. The error rate is not a smooth decline but it is reducing nonetheless. Through fine-tuning, it reaches an error rate of 1.65% on the test set which proves the effectiveness of Wav2Vec.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw79SL7DI6X9"
      },
      "source": [
        "![image.png](attachment:cb89a37c-a031-4db2-b26e-545f9042b119.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9gL2GdsI6YF"
      },
      "source": [
        "**It can be observed that fine-tuning Wav2Vec enhances the performance as compared to the model without fine-tuning. Disclaimer, fine-tuning can cause overfitting and should be done in moderation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8lrYq3zI6YF"
      },
      "source": [
        "![image.png](attachment:a1321def-5a52-4c40-824f-73bc40f1bfd5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCevxtySxBBd"
      },
      "source": [
        "## HuBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxsk7i5DI6YG"
      },
      "source": [
        "**What is HuBERT and why use it?**\n",
        "**Very similar to Wav2Vec in terms of architecture. Performs some more processing steps before training like k-means clustering on fbank features. Like Wav2Vec, it performs very well for speech tasks. It is a good competitor to Wav2Vec, hence experimented.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb8dKRXDI6YG"
      },
      "source": [
        "**Fine-tuning the Conv layers has little to no effect. However, fine-tuning the remaining layers of HuBERT improves the performance of the system.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhXgMUbCDrX6"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ukHA4eDslg",
        "outputId": "f9033f23-b6a6-4f7f-89db-8c5a09a60e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_hubert.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_hubert.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        outputs = self.modules.ssl_model(wavs, lens)\n",
        "\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "    # print(f\"train_data type: {type(train_data)}\")\n",
        "    # print(f\"dataset[0]: {train_data[0]}\")\n",
        "    # print(f\"len of dataset: {len(train_data)}\")\n",
        "    # print(train_data[:5])\n",
        "    # print(f\"type of label_enc: {type(label_encoder)}\")\n",
        "    # print(f\"dataset[0]: {label_encoder[0]}\")\n",
        "    # print(f\"len of dataset: {len(label_encoder)}\")\n",
        "    # print(label_encoder[:5])\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf1TFVppxKY8"
      },
      "source": [
        "### hparams - freeze Conv layers and fine-tune the remaining layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb9Q5j-PEEqt",
        "outputId": "ecc5ce17-9b02-4f49-c46b-88d6f7ee30b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_hubert.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_hubert.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/hubert_1_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/hubert-base-ls960\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8XdnwW1ELAE"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9rX4LycEFYl",
        "outputId": "a61b73a9-ba95-471b-f7d0-0321d493d5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████| 1.39k/1.39k [00:00<00:00, 258kB/s]\n",
            "pytorch_model.bin: 100%|██████████████████████| 378M/378M [00:03<00:00, 111MB/s]\n",
            "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "preprocessor_config.json: 100%|████████████████| 213/213 [00:00<00:00, 43.9kB/s]\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/hubert_1_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 90.2M\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 95.5495% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100%|█████████████████████| 1155/1155 [03:11<00:00,  6.03it/s, train_loss=0.378]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.37it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.78e-01 - valid loss: 1.01e-01, valid ErrorRate: 2.69e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-22-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|█████████████████████| 1155/1155 [03:09<00:00,  6.10it/s, train_loss=0.121]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.61it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.21e-01 - valid loss: 9.64e-02, valid ErrorRate: 2.19e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-25-23+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-22-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|█████████████████████| 1155/1155 [03:09<00:00,  6.09it/s, train_loss=0.095]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.09it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 9.50e-02 - valid loss: 8.83e-02, valid ErrorRate: 2.23e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-28-44+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|████████████████████| 1155/1155 [03:09<00:00,  6.09it/s, train_loss=0.0774]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.73it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 7.74e-02 - valid loss: 9.15e-02, valid ErrorRate: 2.03e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-32-04+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-28-44+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-25-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|████████████████████| 1155/1155 [03:09<00:00,  6.10it/s, train_loss=0.0721]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.15it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 7.21e-02 - valid loss: 7.70e-02, valid ErrorRate: 1.98e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-35-24+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-32-04+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_1_v12/1986/save/CKPT+2024-04-26+01-35-24+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:10<00:00, 14.32it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 6.80e-02, test ErrorRate: 1.67e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_hubert.py hparams_hubert.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHQwlknDuWc"
      },
      "source": [
        "### hparams - fine-tune the Conv layers and freeze the remaining layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JznuOtKEMoz",
        "outputId": "5b434cbe-b443-445b-d7ec-7c8bb2127340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_hubert.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_hubert.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/hubert_2_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/hubert-base-ls960\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: False\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGD4ixHEENPp"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgyz7NDvEM-j",
        "outputId": "171ca772-e8fe-411c-b34e-7f2c12f9513f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████| 1.39k/1.39k [00:00<00:00, 284kB/s]\n",
            "pytorch_model.bin: 100%|██████████████████████| 378M/378M [00:02<00:00, 169MB/s]\n",
            "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - HubertModel is frozen.\n",
            "preprocessor_config.json: 100%|████████████████| 213/213 [00:00<00:00, 45.6kB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/hubert_2_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 9.2k\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 0.0098% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100%|██████████████████████| 1155/1155 [01:16<00:00, 15.13it/s, train_loss=1.57]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.91it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.57 - valid loss: 8.62e-01, valid ErrorRate: 1.16e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-41-18+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.47it/s, train_loss=0.841]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.10it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 8.41e-01 - valid loss: 5.41e-01, valid ErrorRate: 1.06e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-42-42+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-41-18+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.45it/s, train_loss=0.613]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.60it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 6.13e-01 - valid loss: 4.27e-01, valid ErrorRate: 9.77e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-44-07+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-42-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.53it/s, train_loss=0.512]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.77it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.12e-01 - valid loss: 3.66e-01, valid ErrorRate: 8.99e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-45-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-44-07+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.54it/s, train_loss=0.449]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.31it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 4.49e-01 - valid loss: 3.30e-01, valid ErrorRate: 8.40e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-46-55+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-45-31+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_2_v12/1986/save/CKPT+2024-04-26+01-46-55+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:10<00:00, 14.61it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 3.29e-01, test ErrorRate: 8.43e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_hubert.py hparams_hubert.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cxmxVKlDuPd"
      },
      "source": [
        "### hparams - freeze all the layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reS9-tbqEPCH",
        "outputId": "85b5d279-0d1d-46ec-b5f2-b52f351d5d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_hubert.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_hubert.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/hubert_3_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/hubert-base-ls960\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U71UiB4LEPaJ"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DO2VRUaEO_X",
        "outputId": "d418f79c-6b36-4b38-f3bf-76d05a246fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - HubertModel is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/hubert_3_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 9.2k\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 0.0098% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_3_v12/1986/save/CKPT+2024-04-26+01-59-19+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100%|█████████████████████| 1155/1155 [01:16<00:00, 15.03it/s, train_loss=0.405]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.56it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 4.05e-01 - valid loss: 3.10e-01, valid ErrorRate: 8.54e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-24-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100%|█████████████████████| 1155/1155 [01:13<00:00, 15.67it/s, train_loss=0.384]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.65it/s]\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 3.84e-01 - valid loss: 2.95e-01, valid ErrorRate: 8.15e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-25-35+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-24-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+01-59-19+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100%|█████████████████████| 1155/1155 [01:15<00:00, 15.32it/s, train_loss=0.364]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.73it/s]\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 3.64e-01 - valid loss: 2.79e-01, valid ErrorRate: 7.67e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-27-00+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-25-35+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.45it/s, train_loss=0.352]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.06it/s]\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 3.52e-01 - valid loss: 2.65e-01, valid ErrorRate: 7.42e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-28-24+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-27-00+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100%|██████████████████████| 1155/1155 [01:14<00:00, 15.60it/s, train_loss=0.34]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.98it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 3.40e-01 - valid loss: 2.66e-01, valid ErrorRate: 7.65e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-29-48+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|██████████████████████| 1155/1155 [01:15<00:00, 15.25it/s, train_loss=0.33]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.88it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 3.30e-01 - valid loss: 2.67e-01, valid ErrorRate: 7.90e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-31-14+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-29-48+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.56it/s, train_loss=0.318]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.03it/s]\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 3.18e-01 - valid loss: 2.53e-01, valid ErrorRate: 7.28e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-32-37+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-28-24+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-31-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.47it/s, train_loss=0.311]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.66it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-06 to 6.6e-06\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 3.11e-01 - valid loss: 2.58e-01, valid ErrorRate: 7.81e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-34-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.50it/s, train_loss=0.308]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.08it/s]\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 6.56e-05, ssl_lr: 6.56e-06 - train loss: 3.08e-01 - valid loss: 2.50e-01, valid ErrorRate: 7.42e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-35-26+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-34-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|█████████████████████| 1155/1155 [01:13<00:00, 15.63it/s, train_loss=0.305]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.64it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-05 to 5.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-06 to 5.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 6.56e-05, ssl_lr: 6.56e-06 - train loss: 3.05e-01 - valid loss: 2.49e-01, valid ErrorRate: 7.47e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-36-50+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-35-26+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|███████████████████████| 1155/1155 [01:14<00:00, 15.43it/s, train_loss=0.3]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 13.93it/s]\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 5.90e-05, ssl_lr: 5.90e-06 - train loss: 3.00e-01 - valid loss: 2.43e-01, valid ErrorRate: 7.26e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-38-15+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-32-37+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-36-50+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.45it/s, train_loss=0.291]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.91it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-05 to 5.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-06 to 5.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 5.90e-05, ssl_lr: 5.90e-06 - train loss: 2.91e-01 - valid loss: 2.43e-01, valid ErrorRate: 7.47e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-39-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|█████████████████████| 1155/1155 [01:15<00:00, 15.35it/s, train_loss=0.293]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.62it/s]\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 2.93e-01 - valid loss: 2.43e-01, valid ErrorRate: 7.24e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-41-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-38-15+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-39-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|█████████████████████| 1155/1155 [01:14<00:00, 15.57it/s, train_loss=0.291]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.62it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-06 to 4.8e-06\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 2.91e-01 - valid loss: 2.36e-01, valid ErrorRate: 7.24e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-42-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-41-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|█████████████████████| 1155/1155 [01:16<00:00, 15.04it/s, train_loss=0.287]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.28it/s]\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 4.78e-05, ssl_lr: 4.78e-06 - train loss: 2.87e-01 - valid loss: 2.38e-01, valid ErrorRate: 7.22e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-43-56+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-42-29+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_3_v12/1986/save/CKPT+2024-04-26+02-43-56+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:10<00:00, 14.22it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 20 - test loss: 2.27e-01, test ErrorRate: 6.97e-02\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_hubert.py hparams_hubert.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow0izwH4I6Ya"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weLvgkpfI6Ya"
      },
      "source": [
        "![image.png](attachment:0ad5b2bb-c53f-481b-bd57-d8ea413d6c56.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2u2NXSsI6Yb"
      },
      "source": [
        "**The model does not overfit and generalizes well. The error rate also gradually decreases with the epoch count. However, it soon starts to pleateau around the 20th epoch. The final error rate on the test set is 6.97%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puj3nfr_DuLa"
      },
      "source": [
        "### hparams - fine-tune all the layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaJyPrvGEQ11",
        "outputId": "0ca59977-97ba-4663-f25b-b30fb259c393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_hubert.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_hubert.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/hubert_4_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/hubert-base-ls960\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 768\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: False\n",
        "\n",
        "# # Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRWHDwOhERL9"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGcTaNw9EQzL",
        "outputId": "6ac881f6-7225-4ced-a44f-29b3e66d81cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/hubert_4_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 94.4M\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-21-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100%|████████████████████| 1155/1155 [03:26<00:00,  5.58it/s, train_loss=0.0562]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.53it/s]\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.62e-02 - valid loss: 8.79e-02, valid ErrorRate: 1.84e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-48-39+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-21-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.0499]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.92it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 4.99e-02 - valid loss: 9.85e-02, valid ErrorRate: 1.87e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-52-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100%|████████████████████| 1155/1155 [03:23<00:00,  5.69it/s, train_loss=0.0438]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.20it/s]\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 4.38e-02 - valid loss: 8.88e-02, valid ErrorRate: 1.75e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-55-46+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-48-39+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-52-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.0394]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.01it/s]\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 3.94e-02 - valid loss: 9.03e-02, valid ErrorRate: 1.57e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-59-19+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-55-46+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.0404]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.42it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 4.04e-02 - valid loss: 1.01e-01, valid ErrorRate: 1.87e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-02-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|█████████████████████| 1155/1155 [03:22<00:00,  5.71it/s, train_loss=0.037]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.62it/s]\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 3.70e-02 - valid loss: 9.31e-02, valid ErrorRate: 1.66e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-06-25+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-02-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|████████████████████| 1155/1155 [03:23<00:00,  5.68it/s, train_loss=0.0331]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.39it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 3.31e-02 - valid loss: 1.07e-01, valid ErrorRate: 1.78e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-10-00+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-06-25+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|████████████████████| 1155/1155 [03:23<00:00,  5.69it/s, train_loss=0.0276]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.79it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-06 to 6.6e-06\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 2.76e-02 - valid loss: 1.12e-01, valid ErrorRate: 1.78e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-13-33+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-10-00+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.0255]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.48it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-05 to 5.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-06 to 5.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 6.56e-05, ssl_lr: 6.56e-06 - train loss: 2.55e-02 - valid loss: 1.16e-01, valid ErrorRate: 1.80e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-17-06+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-13-33+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.0243]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.90it/s]\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 5.90e-05, ssl_lr: 5.90e-06 - train loss: 2.43e-02 - valid loss: 1.20e-01, valid ErrorRate: 1.73e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-20-39+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-17-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.71it/s, train_loss=0.0231]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.62it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-05 to 5.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-06 to 5.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 5.90e-05, ssl_lr: 5.90e-06 - train loss: 2.31e-02 - valid loss: 1.27e-01, valid ErrorRate: 1.89e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-24-12+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-20-39+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|█████████████████████| 1155/1155 [03:22<00:00,  5.70it/s, train_loss=0.021]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.72it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-06 to 4.8e-06\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 2.10e-02 - valid loss: 1.19e-01, valid ErrorRate: 1.91e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-27-46+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-24-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|████████████████████| 1155/1155 [03:23<00:00,  5.69it/s, train_loss=0.0204]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.64it/s]\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 4.78e-05, ssl_lr: 4.78e-06 - train loss: 2.04e-02 - valid loss: 1.16e-01, valid ErrorRate: 1.78e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-31-20+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-27-46+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|████████████████████| 1155/1155 [03:21<00:00,  5.72it/s, train_loss=0.0194]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 15.15it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-05 to 4.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-06 to 4.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 4.78e-05, ssl_lr: 4.78e-06 - train loss: 1.94e-02 - valid loss: 1.13e-01, valid ErrorRate: 1.78e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-34-52+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-31-20+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|████████████████████| 1155/1155 [03:22<00:00,  5.71it/s, train_loss=0.0151]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.42it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-05 to 3.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-06 to 3.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 4.30e-05, ssl_lr: 4.30e-06 - train loss: 1.51e-02 - valid loss: 1.27e-01, valid ErrorRate: 1.82e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-38-25+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/hubert_4_v12/1986/save/CKPT+2024-04-26+03-34-52+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/hubert_4_v12/1986/save/CKPT+2024-04-26+02-59-19+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:11<00:00, 13.33it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 9 - test loss: 6.67e-02, test ErrorRate: 1.23e-02\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_hubert.py hparams_hubert.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mHWDYNFI6Yg"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8xQ5mvhI6Yh"
      },
      "source": [
        "**Similar to Wav2Vec, the model seems to overfit slightly on the data even though we just use a simple linear layer as a classifier. This is because we are fine-tuning all the layers of HuBERT on the downstream task on a relatively small dataset. With more data, this problem can be tackled. Again, ideally the training would stop at the 5th epoch. The error rate is not a smooth decline but it is reducing nonetheless. Through fine-tuning, it reaches an error rate of 1.23% on the test set on HuBERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZoFHMO-I6Yh"
      },
      "source": [
        "![image.png](attachment:4280191b-ac2a-4d3c-8122-003cb5144555.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o2sjVi_I6Yi"
      },
      "source": [
        "**It can be observed that fine-tuning HuBERT enhances the performance as compared to the model without fine-tuning. Disclaimer, fine-tuning can cause overfitting and should be done in moderation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0SNB6wMI6Yi"
      },
      "source": [
        "![image.png](attachment:1fd30b7a-5471-424d-b317-1a1ff1821bc6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0o8Eo3pxCZE"
      },
      "source": [
        "## WAVLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOxLoB5yI6Yj"
      },
      "source": [
        "**What is WavLM and why use it?**\n",
        "**A successor to Wav2Vec as well, WavLM is the newest and best performing self-supervised feature extractor. It has been pre-trained on a larger corpus and some additions to the transfomer segment were made. This shows the evolution of feature extractors in order.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqK26y_WI6Yk"
      },
      "source": [
        "**WavLM specifically is trained only for 5 epochs. This is because they reach near optimal accuracies within a few epochs. There on, we face the problem of overfitting.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC-_84WAFZEW"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QbmgS4HJh0Y",
        "outputId": "c5b9a3af-cced-4d08-fa58-dc3fa06ef0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_wavlm.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_wavlm.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        outputs = self.modules.ssl_model(wavs, lens)\n",
        "\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "    # print(f\"train_data type: {type(train_data)}\")\n",
        "    # print(f\"dataset[0]: {train_data[0]}\")\n",
        "    # print(f\"len of dataset: {len(train_data)}\")\n",
        "    # print(train_data[:5])\n",
        "    # print(f\"type of label_enc: {type(label_encoder)}\")\n",
        "    # print(f\"dataset[0]: {label_encoder[0]}\")\n",
        "    # print(f\"len of dataset: {len(label_encoder)}\")\n",
        "    # print(label_encoder[:5])\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeD1JoaGxP3s"
      },
      "source": [
        "### hparams - freeze Conv layers and fine-tune the remaining layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tGI9XXKJox6",
        "outputId": "5e6a3eec-d036-4d8e-9094-03f30a0fa63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing hparams_wavlm.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wavlm.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wavlm_1_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: microsoft/wavlm-large\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 1024\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7DJpGIbLuRm"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOd4eZOLtQq",
        "outputId": "b7031e26-7b88-4380-ceb1-2542eb40baa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████| 2.22k/2.22k [00:00<00:00, 392kB/s]\n",
            "pytorch_model.bin: 100%|███████████████████| 1.26G/1.26G [00:21<00:00, 59.8MB/s]\n",
            "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "preprocessor_config.json: 100%|████████████████| 214/214 [00:00<00:00, 46.7kB/s]\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wavlm_1_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 311.3M\n",
            "* Total Number of Parameters: 315.5M\n",
            "* Trainable Parameters represent 98.6665% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0%|                                                  | 0/1157 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|█████████████████████| 1157/1157 [09:24<00:00,  2.05it/s, train_loss=0.357]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:23<00:00,  5.88it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.57e-01 - valid loss: 5.55e-02, valid ErrorRate: 1.47e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+03-53-09+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|████████████████████| 1157/1157 [09:23<00:00,  2.05it/s, train_loss=0.0752]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:23<00:00,  5.86it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 7.52e-02 - valid loss: 4.60e-02, valid ErrorRate: 1.22e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-02-59+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+03-53-09+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|████████████████████| 1157/1157 [09:24<00:00,  2.05it/s, train_loss=0.0599]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:23<00:00,  5.90it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.99e-02 - valid loss: 4.18e-02, valid ErrorRate: 1.15e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-12-51+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-02-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|████████████████████| 1157/1157 [09:25<00:00,  2.05it/s, train_loss=0.0518]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:23<00:00,  5.87it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.18e-02 - valid loss: 4.37e-02, valid ErrorRate: 1.15e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-22-44+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-12-51+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|████████████████████| 1157/1157 [09:24<00:00,  2.05it/s, train_loss=0.0484]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:23<00:00,  5.93it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 4.84e-02 - valid loss: 4.25e-02, valid ErrorRate: 1.24e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-32-35+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wavlm_1_v12/1986/save/CKPT+2024-04-26+04-22-44+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:26<00:00,  5.87it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 4 - test loss: 3.08e-02, test ErrorRate: 9.45e-03\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_wavlm.py hparams_wavlm.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voDvDqe7I6Yq"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_A4ORB5I6a-"
      },
      "source": [
        "![image.png](attachment:bf81ee16-e01d-4e4e-929c-4bcae01e0a65.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMIsy9rHFa65"
      },
      "source": [
        "### hparams - fine-tune the Conv layers and freeze the remaining layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY4oFXUeLj56",
        "outputId": "6b8a19e7-5536-4dad-b18d-4fe240cf20c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_wavlm.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wavlm.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wavlm_2_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: microsoft/wavlm-large\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 1024\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41x9ZoXML2aA"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUc1pRtgL3df",
        "outputId": "7b79f502-055d-4de9-ef54-363136c3ee7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████| 2.22k/2.22k [00:00<00:00, 258kB/s]\n",
            "pytorch_model.bin: 100%|████████████████████| 1.26G/1.26G [00:03<00:00, 329MB/s]\n",
            "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - WavLMModel is frozen.\n",
            "preprocessor_config.json: 100%|████████████████| 214/214 [00:00<00:00, 28.4kB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wavlm_2_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 12.3k\n",
            "* Total Number of Parameters: 315.5M\n",
            "* Trainable Parameters represent 0.0039% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0%|                                                  | 0/1157 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████████████████| 1157/1157 [03:36<00:00,  5.35it/s, train_loss=1.25]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.00it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.25 - valid loss: 4.49e-01, valid ErrorRate: 5.77e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+06-57-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|█████████████████████| 1157/1157 [03:34<00:00,  5.40it/s, train_loss=0.532]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.04it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.32e-01 - valid loss: 2.44e-01, valid ErrorRate: 3.73e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-02-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+06-57-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|█████████████████████| 1157/1157 [03:35<00:00,  5.38it/s, train_loss=0.379]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.04it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.79e-01 - valid loss: 1.80e-01, valid ErrorRate: 3.48e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-06-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-02-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|█████████████████████| 1157/1157 [03:34<00:00,  5.39it/s, train_loss=0.301]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.03it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.01e-01 - valid loss: 1.51e-01, valid ErrorRate: 3.19e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-10-09+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-06-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|█████████████████████| 1157/1157 [03:34<00:00,  5.40it/s, train_loss=0.275]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.03it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 2.75e-01 - valid loss: 1.36e-01, valid ErrorRate: 2.87e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-14-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-10-09+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wavlm_2_v12/1986/save/CKPT+2024-04-26+07-14-13+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:30<00:00,  5.00it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.20e-01, test ErrorRate: 2.51e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_wavlm.py hparams_wavlm.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRqYnLo8Fa31"
      },
      "source": [
        "### hparams - freeze all the layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ4h7JUeJbs4",
        "outputId": "453abe2e-d26f-4208-9988-14c563ed8b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_wavlm.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wavlm.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wavlm_3_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: microsoft/wavlm-large\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 1024\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Qnbx2wL6Eo"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yYgv8pWL7qS",
        "outputId": "57bc1504-d599-4d71-8656-abf53c911721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - WavLMModel is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wavlm_3_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 12.3k\n",
            "* Total Number of Parameters: 315.5M\n",
            "* Trainable Parameters represent 0.0039% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0%|                                                  | 0/1157 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████████████████| 1157/1157 [03:36<00:00,  5.34it/s, train_loss=1.25]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  4.97it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.25 - valid loss: 4.49e-01, valid ErrorRate: 5.77e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-32-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|█████████████████████| 1157/1157 [03:35<00:00,  5.37it/s, train_loss=0.532]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.02it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 5.32e-01 - valid loss: 2.44e-01, valid ErrorRate: 3.73e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-36-07+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-32-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|█████████████████████| 1157/1157 [03:35<00:00,  5.38it/s, train_loss=0.379]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.03it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.79e-01 - valid loss: 1.80e-01, valid ErrorRate: 3.48e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-40-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-36-07+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|█████████████████████| 1157/1157 [03:35<00:00,  5.37it/s, train_loss=0.301]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.01it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 3.01e-01 - valid loss: 1.51e-01, valid ErrorRate: 3.19e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-44-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-40-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|█████████████████████| 1157/1157 [03:34<00:00,  5.38it/s, train_loss=0.275]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:27<00:00,  5.02it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 2.75e-01 - valid loss: 1.36e-01, valid ErrorRate: 2.87e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-48-20+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-44-16+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wavlm_3_v12/1986/save/CKPT+2024-04-26+06-48-20+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:30<00:00,  5.01it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 5 - test loss: 1.20e-01, test ErrorRate: 2.51e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_wavlm.py hparams_wavlm.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjKf_Yd9I6bB"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y5n9MjbI6bB"
      },
      "source": [
        "**Even with fine-tuning just the output linear layer, it achieves error rates between 5 to 3%. This tells us that WavLM has been trained well and is naturally good at extracting continuous features from audio signals. The final test error rate is 2.5% which is very close to the fine-tuned version of Wav2Vec.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYkdQ6z8I6bC"
      },
      "source": [
        "![image.png](attachment:36351f0c-e40f-4436-b6b7-0b60438325bc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KLg93npFazd"
      },
      "source": [
        "### hparams - fine-tune all the layers of the self-supervised features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdpOMEehKF45",
        "outputId": "cefbb897-d5a3-404f-e8f6-8318111ed406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_wavlm.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wavlm.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/wavlm_4_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: microsoft/wavlm-large\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "lr_final: 0.0001\n",
        "encoder_dim: 1024\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYiz9DLoxPuy"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PcDYOloL9Yn",
        "outputId": "4d764565-5a40-472a-a055-aa462bb1a6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wavlm_4_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "Downloading https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1 to /path/to/GSC/noise/data.zip\n",
            "noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1: 569MB [00:04, 128MB/s]         \n",
            "Extracting /path/to/GSC/noise/data.zip to /path/to/GSC/noise\n",
            "Downloading https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1 to /path/to/GSC/rir/data.zip\n",
            "RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1: 246MB [00:02, 91.5MB/s]          \n",
            "Extracting /path/to/GSC/rir/data.zip to /path/to/GSC/rir\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 315.5M\n",
            "* Total Number of Parameters: 315.5M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wavlm_4_v12/1986/save/CKPT+2024-04-26+05-27-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "  0%|                                                  | 0/1157 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|████████████████████| 1157/1157 [10:43<00:00,  1.80it/s, train_loss=0.0753]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:25<00:00,  5.56it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 7.53e-02 - valid loss: 4.79e-02, valid ErrorRate: 1.31e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+05-53-03+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+05-27-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|████████████████████| 1157/1157 [10:30<00:00,  1.83it/s, train_loss=0.0608]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:24<00:00,  5.59it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 6.08e-02 - valid loss: 5.03e-02, valid ErrorRate: 1.42e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+06-04-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|████████████████████| 1157/1157 [10:31<00:00,  1.83it/s, train_loss=0.0517]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:24<00:00,  5.58it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 5.17e-02 - valid loss: 5.02e-02, valid ErrorRate: 1.27e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+06-15-04+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+05-53-03+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+06-04-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|████████████████████| 1157/1157 [10:32<00:00,  1.83it/s, train_loss=0.0424]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:24<00:00,  5.60it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 4.24e-02 - valid loss: 5.09e-02, valid ErrorRate: 1.31e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wavlm_4_v12/1986/save/CKPT+2024-04-26+06-26-07+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wavlm_4_v12/1986/save/CKPT+2024-04-26+06-15-04+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:27<00:00,  5.47it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 4 - test loss: 2.74e-02, test ErrorRate: 7.60e-03\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/encodec_v12/1986/\n",
        "\n",
        "!python train_wavlm.py hparams_wavlm.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwGcQ4G6I6bF"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldgD36nII6bG"
      },
      "source": [
        "**WavLM performs exceedingly well within a few epochs. The error rate is not smooth but decreases in time. The final test error rate is 0.76% which is amazing.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kL2d_FSI6bG"
      },
      "source": [
        "![image.png](attachment:38cb8211-4778-46c6-9df3-98960d38b3d1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGklludI6bG"
      },
      "source": [
        "#### Wav2Vev vs HuBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykn-JwjiI6bH"
      },
      "source": [
        "**Wav2Vec and WavLM show close results. Wav2Vec seems to converge faster but they both end up at the same error rates later on.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8rgvu3aI6bH"
      },
      "source": [
        "![image.png](attachment:970514cb-5475-4a98-a6b4-60b654dec369.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T_aRbukvq0Y"
      },
      "source": [
        "# DISCRETE FEATURES WITH ENCODEC AND DAC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qSXrGX2I6bI"
      },
      "source": [
        "**What are discrete features? Why are they needed?**\n",
        "**Discrete features in Speech Recognition contain distinct, categorical data about the audio signals. They give us linguistic information like phenomes or words. The main reason why we want to utilize discrete features is due to their compactness. They are more data efficient and robust. We will mainly use Encodec for the experimentation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding for Encodec discrete features"
      ],
      "metadata": {
        "id": "WdJRvBFCzOWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file custom_model.py\n",
        "\n",
        "import torch\n",
        "\n",
        "class AttentionMLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(AttentionMLP, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, 1, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        att_w = torch.nn.functional.softmax(x, dim=2)\n",
        "        return att_w\n",
        "\n",
        "\n",
        "class Discrete_EmbeddingLayer(torch.nn.Module):\n",
        "    \"\"\"This class handles embedding layers  for discrete tokens.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    num_codebooks: int ,\n",
        "        number of codebooks of the tokenizer.\n",
        "    vocab_size : int,\n",
        "        size of the dictionary of embeddings\n",
        "    emb_dim: int ,\n",
        "        the size of each embedding vector\n",
        "    pad_index: int (default: 0),\n",
        "        If specified, the entries at padding_idx do not contribute to the gradient.\n",
        "    init: boolean (default: False):\n",
        "        If set to True, init the embedding with the tokenizer embedding otherwise init randomly.\n",
        "    freeze: boolean (default: False)\n",
        "       If True, the embedding is frozen. If False, the model will be trained\n",
        "        alongside with the rest of the pipeline.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from speechbrain.lobes.models.huggingface_transformers.encodec import Encodec\n",
        "    >>> model_hub = \"facebook/encodec_24khz\"\n",
        "    >>> save_path = \"savedir\"\n",
        "    >>> model = Encodec(model_hub, save_path)\n",
        "    >>> audio = torch.randn(4, 1000)\n",
        "    >>> length = torch.tensor([1.0, .5, .75, 1.0])\n",
        "    >>> tokens, emb = model.encode(audio, length)\n",
        "    >>> print(tokens.shape)\n",
        "    torch.Size([4, 4, 2])\n",
        "    >>> emb= Discrete_EmbeddingLayer(2, 1024, 1024)\n",
        "    >>> in_emb = emb(tokens)\n",
        "    >>> print(in_emb.shape)\n",
        "    torch.Size([4, 4, 2, 1024])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_codebooks,\n",
        "        vocab_size,\n",
        "        emb_dim,\n",
        "        pad_index=0,\n",
        "        init=False,\n",
        "        freeze=False,\n",
        "    ):\n",
        "        super(Discrete_EmbeddingLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_codebooks = num_codebooks\n",
        "        self.freeze = freeze\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            num_codebooks * vocab_size, emb_dim\n",
        "        ).requires_grad_(not self.freeze)\n",
        "        self.init= init\n",
        "\n",
        "\n",
        "    def init_embedding(self,weights):\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight = torch.nn.Parameter(weights)\n",
        "\n",
        "    def forward(self, in_tokens):\n",
        "        \"\"\"Computes the embedding for discrete tokens.\n",
        "        a sample.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        in_tokens : torch.Tensor\n",
        "            A (Batch x Time x num_codebooks)\n",
        "            audio sample\n",
        "        Returns\n",
        "        -------\n",
        "        in_embs : torch.Tensor\n",
        "        \"\"\"\n",
        "        with torch.set_grad_enabled(not self.freeze):\n",
        "            #  Add unique token IDs across diffrent codebooks by adding num_codebooks * vocab_size\n",
        "            in_tokens += torch.arange(\n",
        "                0,\n",
        "                self.num_codebooks * self.vocab_size,\n",
        "                self.vocab_size,\n",
        "                device=in_tokens.device,\n",
        "            )\n",
        "            # Forward Pass to embedding and\n",
        "            in_embs = self.embedding(in_tokens)\n",
        "            return in_embs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzfRgsOozNMM",
        "outputId": "a36b42fa-e94c-4a6b-e5c9-db36ee7db4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvkGFSKfhXgY"
      },
      "source": [
        "## ENCODEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhhDR-Zk2W3y"
      },
      "source": [
        "**Freezing or fine-tuning Encodec results in the same performance. So we shall freeze the Encodec's layers during this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRIzmzYuNouk"
      },
      "source": [
        "## hparams - Xvector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFUbS5H4I6bJ"
      },
      "source": [
        "**Xvector intuitively will not perform well because it is just a CNN module applied on our extracted features. It just learns the local dependencies and lose out on the global dependencies. We choose this model to set low standards and compare how RNNs and Transformers perform vis-a-vis.**\n",
        "\n",
        "**Experimented with multiple combinations of convolution layers and kernels. The error rate does not reduce substantially but we can preclude overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4tBrkpkV4XP",
        "outputId": "1e6dac4a-5a45-4e0a-d301-15cbce0d9148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_encodec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_encodec.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/encodec_Xvector_<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/encodec_24khz\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequencies for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "\n",
        "sample_rate: 24000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: True\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "# freeze_ssl_conv: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 4\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <sslmodel_hub>\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    bandwidth: 6.0\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    flat_embeddings: False\n",
        "    renorm_embeddings: True\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "num_codebooks: 8\n",
        "num_clusters: 1024\n",
        "encoder_dim: 256\n",
        "\n",
        "discrete_embedding_layer: !new:custom_model.Discrete_EmbeddingLayer\n",
        "   num_codebooks: !ref <num_codebooks>\n",
        "   vocab_size: !ref <num_clusters>\n",
        "   emb_dim: !ref <encoder_dim>\n",
        "\n",
        "attention_mlp: !new:custom_model.AttentionMLP\n",
        "    input_dim: !ref <encoder_dim>\n",
        "    hidden_dim: !ref <encoder_dim>\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "# Xvector model parameters\n",
        "\n",
        "# emb_dim: 64 #128\n",
        "\n",
        "#Xvector Model\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <encoder_dim>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 1 #2\n",
        "    tdnn_channels: [64, 64]\n",
        "    tdnn_kernel_sizes: [3, 3]\n",
        "    tdnn_dilations: [ 1, 1]\n",
        "    lin_neurons: !ref <encoder_dim>\n",
        "\n",
        "# Clasifier applied on top of the embeddings\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, !ref <encoder_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <encoder_dim>\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    discrete_embedding_layer: !ref <discrete_embedding_layer>\n",
        "    attention_mlp: !ref <attention_mlp>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <ssl_model>, !ref <embedding_model> ,!ref <classifier>]\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing_output: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpWm9vlqVqjW"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYb0oZYLiORp",
        "outputId": "0c2d8e8d-486c-419a-a04f-7c7d1e419b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_encodec.py\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "import torchaudio\n",
        "# import librosa\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\"\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "        # print(f' \\n computefeature type :{type(self.modules.compute_features)}')\n",
        "        # if   isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "            # Feature extraction and normalization\n",
        "        with torch.no_grad():\n",
        "          self.modules.ssl_model.to(self.device).eval()\n",
        "          tokens,_ = self.modules.ssl_model.encode(wavs, lens)\n",
        "          # print(f\"wavs shape: {wavs.shape}\")\n",
        "          # print(f\"tokens shape: {tokens.shape}\")\n",
        "\n",
        "        input_embeddings = self.modules.discrete_embedding_layer(tokens)\n",
        "        # input_embeddings shape: torch.Size([5, 693, 8, 512])\n",
        "\n",
        "        input_att_w = self.modules.attention_mlp(input_embeddings)\n",
        "        input_feats = torch.matmul(input_att_w.transpose(2, -1), input_embeddings).squeeze(-2)\n",
        "        # tokens= tokens.float()\n",
        "        outputs = self.hparams.mean_var_norm(input_feats, lens)\n",
        "        embeddings = self.modules.embedding_model(outputs, lens)\n",
        "        # embeddings = self.hparams.avg_pool(embeddings,lens)\n",
        "        predictions = self.modules.classifier(embeddings)\n",
        "\n",
        "\n",
        "        # # Ecapa model uses softmax outside of its classifer\n",
        "        # if \"softmax\" in self.modules.keys():\n",
        "        #     outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return predictions, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\n",
        "        \"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # command = command.squeeze(1)\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command,lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "    def init_optimizers(self):\n",
        "      \"Initializes the ssl optimizer and model optimizer\"\n",
        "      self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "          self.modules.ssl_model.parameters()\n",
        "      )\n",
        "      self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "      if self.checkpointer is not None:\n",
        "          self.checkpointer.add_recoverable(\n",
        "              \"ssl_opt\", self.ssl_optimizer\n",
        "          )\n",
        "          self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "      self.optimizers_dict = {\n",
        "          \"model_optimizer\": self.optimizer,\n",
        "          \"ssl_optimizer\": self.ssl_optimizer,\n",
        "      }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration, target_sr=24000):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file, from_didatasets=[train_data], output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    # sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    # sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for 12.0 - 16 codebooks\n",
        "# for 6.0 - 8 codebooks"
      ],
      "metadata": {
        "id": "qkIv48Ho8oK7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSxO-uF-VrZc"
      },
      "source": [
        "\n",
        "### Run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMr82GHnk0NF",
        "outputId": "64902676-3363-434b-8337-066c484757e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 809/809 [00:00<00:00, 5.23MB/s]\n",
            "model.safetensors: 100% 93.1M/93.1M [00:02<00:00, 34.2MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_Xvector_12/1986\n",
            "numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "Downloading http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz to /path/to/GSC/speech_commands_v0.02.tar.gz\n",
            "speech_commands_v0.02.tar.gz: 2.43GB [01:59, 20.4MB/s]                \n",
            "Extracting /path/to/GSC/speech_commands_v0.02.tar.gz to /path/to/GSC\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "/usr/local/lib/python3.10/dist-packages/speechbrain/core.py:789: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=gradscaler_enabled)\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 2.3M\n",
            "* Total Number of Parameters: 17.2M\n",
            "* Trainable Parameters represent 13.4877% of the total size.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 1153/1153 [03:39<00:00,  5.24it/s, train_loss=2.13]\n",
            "100% 140/140 [00:23<00:00,  5.96it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04 - train loss: 2.13 - valid loss: 1.85, valid ErrorRate: 6.33e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-50-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 1153/1153 [03:38<00:00,  5.28it/s, train_loss=1.67]\n",
            "100% 140/140 [00:23<00:00,  6.07it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04 - train loss: 1.67 - valid loss: 1.61, valid ErrorRate: 5.52e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-54-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-50-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 1153/1153 [03:36<00:00,  5.33it/s, train_loss=1.46]\n",
            "100% 140/140 [00:23<00:00,  6.04it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 9.00e-05 - train loss: 1.46 - valid loss: 1.53, valid ErrorRate: 5.24e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-58-06+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-54-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 1153/1153 [03:35<00:00,  5.34it/s, train_loss=1.32]\n",
            "100% 140/140 [00:23<00:00,  5.99it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 8.10e-05 - train loss: 1.32 - valid loss: 1.50, valid ErrorRate: 5.18e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-02-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+02-58-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 1153/1153 [03:35<00:00,  5.35it/s, train_loss=1.22]\n",
            "100% 140/140 [00:23<00:00,  5.93it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 7.29e-05 - train loss: 1.22 - valid loss: 1.51, valid ErrorRate: 5.11e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-06-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-02-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100% 1153/1153 [03:34<00:00,  5.38it/s, train_loss=1.13]\n",
            "100% 140/140 [00:24<00:00,  5.63it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-05 to 5.9e-05\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 6.56e-05 - train loss: 1.13 - valid loss: 1.51, valid ErrorRate: 5.10e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-10-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-06-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100% 1153/1153 [03:35<00:00,  5.34it/s, train_loss=1.06]\n",
            "100% 140/140 [00:26<00:00,  5.28it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-05 to 5.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 5.90e-05 - train loss: 1.06 - valid loss: 1.54, valid ErrorRate: 5.11e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-14-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100% 1153/1153 [03:33<00:00,  5.41it/s, train_loss=0.997]\n",
            "100% 140/140 [00:24<00:00,  5.80it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 5.31e-05 - train loss: 9.97e-01 - valid loss: 1.56, valid ErrorRate: 5.15e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-18-06+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-14-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100% 1153/1153 [03:39<00:00,  5.24it/s, train_loss=0.941]\n",
            "100% 140/140 [00:23<00:00,  6.03it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-05 to 4.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 4.78e-05 - train loss: 9.41e-01 - valid loss: 1.58, valid ErrorRate: 5.12e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-22-10+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-18-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100% 1153/1153 [03:37<00:00,  5.31it/s, train_loss=0.897]\n",
            "100% 140/140 [00:23<00:00,  5.94it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-05 to 3.9e-05\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 4.30e-05 - train loss: 8.97e-01 - valid loss: 1.61, valid ErrorRate: 5.16e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-26-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-22-10+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100% 1153/1153 [03:37<00:00,  5.31it/s, train_loss=0.859]\n",
            "100% 140/140 [00:23<00:00,  5.96it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.9e-05 to 3.5e-05\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 3.87e-05 - train loss: 8.59e-01 - valid loss: 1.64, valid ErrorRate: 5.10e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-30-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-26-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-10-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100% 1153/1153 [03:35<00:00,  5.34it/s, train_loss=0.818]\n",
            "100% 140/140 [00:24<00:00,  5.74it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.5e-05 to 3.1e-05\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 3.49e-05 - train loss: 8.18e-01 - valid loss: 1.68, valid ErrorRate: 5.21e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_Xvector_12/1986/save/CKPT+2024-09-02+03-34-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            " 68% 789/1153 [02:25<01:07,  5.40it/s, train_loss=0.778]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_encodec.py\", line 334, in <module>\n",
            "    speaker_brain.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1607, in fit\n",
            "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1432, in _fit_train\n",
            "    loss = self.fit_batch(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1226, in fit_batch\n",
            "    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
            "  File \"/content/train_encodec.py\", line 58, in compute_forward\n",
            "    tokens,_ = self.modules.ssl_model.encode(wavs, lens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/huggingface_transformers/encodec.py\", line 237, in encode\n",
            "    tokens = self._encode_tokens(inputs, length)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/huggingface_transformers/encodec.py\", line 263, in _encode_tokens\n",
            "    result = self.model.encode(inputs, mask, bandwidth=self.bandwidth)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 642, in encode\n",
            "    encoded_frame, scale = self._encode_frame(frame, bandwidth, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 575, in _encode_frame\n",
            "    embeddings = self.encoder(input_values)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 303, in forward\n",
            "    hidden_states = layer(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 158, in forward\n",
            "    hidden_states = self._pad1d(hidden_states, (self.padding_total, extra_padding), mode=self.pad_mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 144, in _pad1d\n",
            "    max_pad = max(padding_left, padding_right)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "!rm -rf /content/results/encodec_Xvector_12/1986/\n",
        "\n",
        "!python train_encodec.py hparams_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrJVjUfI6bM"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZUgXsVdI6bM"
      },
      "source": [
        "**As said earlier, we see that Xvector overfits on the input data. The error rate also is pretty high since it does not capture long term dependencies. The final test error rate is around 73%. With this, we know that Xvector or CNN by itself is not a good idea for audio signals.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFE7ynZnI6bN"
      },
      "source": [
        "![image.png](attachment:3e3bb128-44c3-4666-947f-a8164e32da6e.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLjtAcUsI6bN"
      },
      "source": [
        "### train with augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj7jARaVI6bN",
        "outputId": "f8f31fe3-d3a4-4e50-9d78-5e322e8747de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_encodec.py\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "import torchaudio\n",
        "# import librosa\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\"\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "        # print(f' \\n computefeature type :{type(self.modules.ssl_model)}')\n",
        "        if   isinstance(\n",
        "            self.modules.ssl_model, speechbrain.lobes.features.Leaf\n",
        "        ):\n",
        "            # if leaf, first normalize the wavs before feeding them to leaf\n",
        "            # no normalization is needed after LEAF\n",
        "            feats = self.hparams.mean_var_norm(wavs, lens)\n",
        "            tokens,_ = self.modules.ssl_model(feats, lens)\n",
        "            tokens = tokens.float()\n",
        "        else:\n",
        "            tokens,_ = self.modules.ssl_model(wavs, lens)\n",
        "            tokens = tokens.float()\n",
        "            tokens = self.hparams.mean_var_norm(tokens, lens)\n",
        "\n",
        "        embeddings = self.modules.embedding_model(tokens, lens)\n",
        "        predictions = self.modules.classifier(embeddings)\n",
        "\n",
        "\n",
        "        # # Ecapa model uses softmax outside of its classifer\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return predictions, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\n",
        "        \"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "        # print(f'command shape :{command.shape}')\n",
        "        # print(f'/n/n predictions shpae :{predictions.shape} and lens : {lens.shape}')\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # command = command.squeeze(1)\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command,lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "    def init_optimizers(self):\n",
        "      \"Initializes the ssl optimizer and model optimizer\"\n",
        "      self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "          self.modules.ssl_model.parameters()\n",
        "      )\n",
        "      self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "      if self.checkpointer is not None:\n",
        "          self.checkpointer.add_recoverable(\n",
        "              \"ssl_opt\", self.ssl_optimizer\n",
        "          )\n",
        "          self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "      self.optimizers_dict = {\n",
        "          \"model_optimizer\": self.optimizer,\n",
        "          \"ssl_optimizer\": self.ssl_optimizer,\n",
        "      }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration,target_sr=24000):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file, from_didatasets=[train_data], output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrZ8kHhoI6bO"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEbRsQ83I6bO",
        "outputId": "e41dfb32-e168-4c62-f1aa-42da7af8d40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_aug_Xvector12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 29.2k\n",
            "* Total Number of Parameters: 14.9M\n",
            "* Trainable Parameters represent 0.1962% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100%|██████████████████████| 1152/1152 [09:58<00:00,  1.93it/s, train_loss=2.44]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 17.44it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04 - train loss: 2.44 - valid loss: 2.30, valid ErrorRate: 7.99e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-26+23-45-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|██████████████████████| 1152/1152 [09:50<00:00,  1.95it/s, train_loss=2.34]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.31it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04 - train loss: 2.34 - valid loss: 2.22, valid ErrorRate: 7.67e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-26+23-55-12+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-26+23-45-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|██████████████████████| 1152/1152 [09:50<00:00,  1.95it/s, train_loss=2.29]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 19.03it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 9.00e-05 - train loss: 2.29 - valid loss: 2.17, valid ErrorRate: 7.50e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-05-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-26+23-55-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|██████████████████████| 1152/1152 [09:54<00:00,  1.94it/s, train_loss=2.26]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.03it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 8.10e-05 - train loss: 2.26 - valid loss: 2.15, valid ErrorRate: 7.36e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-15-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-05-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|██████████████████████| 1152/1152 [09:51<00:00,  1.95it/s, train_loss=2.24]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.93it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 7.29e-05 - train loss: 2.24 - valid loss: 2.12, valid ErrorRate: 7.29e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-25-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-15-13+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100%|██████████████████████| 1152/1152 [09:56<00:00,  1.93it/s, train_loss=2.22]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 19.07it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-05 to 5.9e-05\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 6.56e-05 - train loss: 2.22 - valid loss: 2.11, valid ErrorRate: 7.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-35-18+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-25-13+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100%|██████████████████████| 1152/1152 [09:57<00:00,  1.93it/s, train_loss=2.21]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.34it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-05 to 5.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 5.90e-05 - train loss: 2.21 - valid loss: 2.09, valid ErrorRate: 7.19e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-45-23+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-35-18+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100%|███████████████████████| 1152/1152 [09:55<00:00,  1.94it/s, train_loss=2.2]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.67it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 5.31e-05 - train loss: 2.20 - valid loss: 2.08, valid ErrorRate: 7.14e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-55-26+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-45-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100%|██████████████████████| 1152/1152 [09:57<00:00,  1.93it/s, train_loss=2.19]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 19.06it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-05 to 4.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 4.78e-05 - train loss: 2.19 - valid loss: 2.07, valid ErrorRate: 7.06e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-05-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+00-55-26+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100%|██████████████████████| 1152/1152 [10:04<00:00,  1.91it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:08<00:00, 16.54it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-05 to 3.9e-05\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 4.30e-05 - train loss: 2.18 - valid loss: 2.06, valid ErrorRate: 7.05e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-15-44+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-05-31+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|██████████████████████| 1152/1152 [10:06<00:00,  1.90it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.21it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.9e-05 to 3.5e-05\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 3.87e-05 - train loss: 2.18 - valid loss: 2.06, valid ErrorRate: 7.02e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-26-00+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-15-44+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|██████████████████████| 1152/1152 [10:02<00:00,  1.91it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.16it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.5e-05 to 3.1e-05\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 3.49e-05 - train loss: 2.18 - valid loss: 2.05, valid ErrorRate: 7.01e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-36-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-26-00+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|██████████████████████| 1152/1152 [10:04<00:00,  1.91it/s, train_loss=2.17]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 17.67it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.1e-05 to 2.8e-05\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 3.14e-05 - train loss: 2.17 - valid loss: 2.05, valid ErrorRate: 7.03e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-46-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|██████████████████████| 1152/1152 [10:00<00:00,  1.92it/s, train_loss=2.17]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.52it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 2.8e-05 to 2.5e-05\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 2.82e-05 - train loss: 2.17 - valid loss: 2.04, valid ErrorRate: 6.99e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-56-32+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-36-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-46-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|██████████████████████| 1152/1152 [09:49<00:00,  1.95it/s, train_loss=2.16]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.68it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 2.5e-05 to 2.3e-05\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 2.54e-05 - train loss: 2.16 - valid loss: 2.04, valid ErrorRate: 6.98e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-06-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+01-56-32+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|██████████████████████| 1152/1152 [09:50<00:00,  1.95it/s, train_loss=2.16]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.38it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 2.3e-05 to 2.1e-05\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 2.29e-05 - train loss: 2.16 - valid loss: 2.04, valid ErrorRate: 7.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-16-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|██████████████████████| 1152/1152 [09:52<00:00,  1.94it/s, train_loss=2.16]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.74it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 2.1e-05 to 1.9e-05\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 2.06e-05 - train loss: 2.16 - valid loss: 2.03, valid ErrorRate: 6.95e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-26-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-16-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-06-30+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|██████████████████████| 1152/1152 [09:51<00:00,  1.95it/s, train_loss=2.15]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.62it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 1.9e-05 to 1.7e-05\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 1.85e-05 - train loss: 2.15 - valid loss: 2.03, valid ErrorRate: 6.93e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-36-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-26-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|██████████████████████| 1152/1152 [09:52<00:00,  1.95it/s, train_loss=2.15]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 18.93it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 1.7e-05 to 1.5e-05\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 1.67e-05 - train loss: 2.15 - valid loss: 2.03, valid ErrorRate: 6.93e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-46-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|██████████████████████| 1152/1152 [09:52<00:00,  1.94it/s, train_loss=2.15]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:07<00:00, 17.64it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 1.5e-05 to 1.4e-05\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 1.50e-05 - train loss: 2.15 - valid loss: 2.03, valid ErrorRate: 6.93e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-56-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-36-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-46-29+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/encodec_aug_Xvector12/1986/save/CKPT+2024-04-27+02-56-29+00\n",
            "100%|█████████████████████████████████████████| 152/152 [00:08<00:00, 18.18it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 20 - test loss: 2.04, test ErrorRate: 7.01e-01\n"
          ]
        }
      ],
      "source": [
        "!python train_encodec.py hparams_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file custom_model.py\n",
        "\n",
        "import torch\n",
        "\n",
        "class AttentionMLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(AttentionMLP, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, 1, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        att_w = torch.nn.functional.softmax(x, dim=2)\n",
        "        return att_w\n",
        "\n",
        "\n",
        "class Discrete_EmbeddingLayer(torch.nn.Module):\n",
        "    \"\"\"This class handles embedding layers  for discrete tokens.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    num_codebooks: int ,\n",
        "        number of codebooks of the tokenizer.\n",
        "    vocab_size : int,\n",
        "        size of the dictionary of embeddings\n",
        "    emb_dim: int ,\n",
        "        the size of each embedding vector\n",
        "    pad_index: int (default: 0),\n",
        "        If specified, the entries at padding_idx do not contribute to the gradient.\n",
        "    init: boolean (default: False):\n",
        "        If set to True, init the embedding with the tokenizer embedding otherwise init randomly.\n",
        "    freeze: boolean (default: False)\n",
        "       If True, the embedding is frozen. If False, the model will be trained\n",
        "        alongside with the rest of the pipeline.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from speechbrain.lobes.models.huggingface_transformers.encodec import Encodec\n",
        "    >>> model_hub = \"facebook/encodec_24khz\"\n",
        "    >>> save_path = \"savedir\"\n",
        "    >>> model = Encodec(model_hub, save_path)\n",
        "    >>> audio = torch.randn(4, 1000)\n",
        "    >>> length = torch.tensor([1.0, .5, .75, 1.0])\n",
        "    >>> tokens, emb = model.encode(audio, length)\n",
        "    >>> print(tokens.shape)\n",
        "    torch.Size([4, 4, 2])\n",
        "    >>> emb= Discrete_EmbeddingLayer(2, 1024, 1024)\n",
        "    >>> in_emb = emb(tokens)\n",
        "    >>> print(in_emb.shape)\n",
        "    torch.Size([4, 4, 2, 1024])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_codebooks,\n",
        "        vocab_size,\n",
        "        emb_dim,\n",
        "        pad_index=0,\n",
        "        init=False,\n",
        "        freeze=False,\n",
        "    ):\n",
        "        super(Discrete_EmbeddingLayer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_codebooks = num_codebooks\n",
        "        self.freeze = freeze\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            num_codebooks * vocab_size, emb_dim\n",
        "        ).requires_grad_(not self.freeze)\n",
        "        self.init= init\n",
        "\n",
        "\n",
        "    def init_embedding(self,weights):\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight = torch.nn.Parameter(weights)\n",
        "\n",
        "    def forward(self, in_tokens):\n",
        "        \"\"\"Computes the embedding for discrete tokens.\n",
        "        a sample.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        in_tokens : torch.Tensor\n",
        "            A (Batch x Time x num_codebooks)\n",
        "            audio sample\n",
        "        Returns\n",
        "        -------\n",
        "        in_embs : torch.Tensor\n",
        "        \"\"\"\n",
        "        with torch.set_grad_enabled(not self.freeze):\n",
        "            #  Add unique token IDs across diffrent codebooks by adding num_codebooks * vocab_size\n",
        "            in_tokens += torch.arange(\n",
        "                0,\n",
        "                self.num_codebooks * self.vocab_size,\n",
        "                self.vocab_size,\n",
        "                device=in_tokens.device,\n",
        "            )\n",
        "            # Forward Pass to embedding and\n",
        "            in_embs = self.embedding(in_tokens)\n",
        "            return in_embs"
      ],
      "metadata": {
        "id": "uxP8gs2z5mbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yvC7_e5CHra"
      },
      "source": [
        "## hparams - LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKiPAV_1I6bP"
      },
      "source": [
        "**What is LSTM? Why use it?**\n",
        "**LSTM is a type of RNN used for sequential data. Unlike the traditional RNN, LSTMs can fight off vanishing gradients better. They can handle long-term dependencies, making them an ideal candidate for speech tasks. Many combinations were tried and tested. Some features that were tuned are batch_size: 32, 64; learning rate: 0.0001-0.0005; hidden_size: 64, 128, 256; number of cells: 2, 3, 4; to name a few. Intuitively they should perform good and way better than Xvectors.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-qOoiQUCRMN",
        "outputId": "cf186444-d321-4504-e619-36f52d597443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_lstm_encodec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_lstm_encodec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/encodec_rnn_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/encodec_24khz\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32 #64 #128\n",
        "lr: 0.00025 #0.0001\n",
        "lr_ssl: 0.00001\n",
        "hidden: 128 #64 #256\n",
        "linear_dim: 256 # 6750 #4800 #9600 #19200 # #38400 #\n",
        "emb_size: 512\n",
        "dropout: 0.5\n",
        "num_codebooks: 8\n",
        "num_clusters: 1024\n",
        "encoder_dim: 128\n",
        "\n",
        "sample_rate: 24000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <sslmodel_hub>\n",
        "    bandwidth: 6 #12\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings: False\n",
        "    freeze: !ref <freeze>\n",
        "    renorm_embeddings: True\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "discrete_embedding_layer: !new:custom_model.Discrete_EmbeddingLayer\n",
        "   num_codebooks: !ref <num_codebooks>\n",
        "   vocab_size: !ref <num_clusters>\n",
        "   emb_dim: !ref <encoder_dim>\n",
        "\n",
        "attention_mlp: !new:custom_model.AttentionMLP\n",
        "   input_dim: !ref <encoder_dim>\n",
        "   hidden_dim: !ref <encoder_dim>\n",
        "\n",
        "\n",
        "lstm: !new:speechbrain.nnet.RNN.LSTM\n",
        "        input_size: !ref <encoder_dim>\n",
        "        bidirectional: False #True\n",
        "        hidden_size: !ref <hidden>\n",
        "        num_layers: 2 #3\n",
        "        dropout: !ref <dropout>\n",
        "        re_init: True\n",
        "\n",
        "linear_1: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: !ref <linear_dim>\n",
        "        n_neurons: !ref <out_n_neurons>\n",
        "        bias: False\n",
        "\n",
        "# linear_2: !new:speechbrain.nnet.linear.Linear\n",
        "#         input_size: 100 #64\n",
        "#         n_neurons: !ref <out_n_neurons>\n",
        "#         bias: False\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: True\n",
        "\n",
        "dropout_layer: !new:speechbrain.nnet.dropout.Dropout2d\n",
        "          drop_rate: !ref <dropout>\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    lstm: !ref <lstm>\n",
        "    linear_1: !ref <linear_1>\n",
        "    # linear_2: !ref <linear_2>\n",
        "    log_softmax: !ref <log_softmax>\n",
        "    discrete_embedding_layer: !ref <discrete_embedding_layer>\n",
        "    attention_mlp: !ref <attention_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <ssl_model>, !ref <lstm>, !ref <linear_1>, !ref <log_softmax>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_wTTYW4CLaC"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE8B13W3CTJ6",
        "outputId": "88cdf696-d31c-4414-84b7-d040c64c8576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_lstm_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_lstm_encodec.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        # print(f\"wavs shape, {wavs.shape}\")\n",
        "        tokens, embeddings = self.modules.ssl_model.encode(wavs, lens)\n",
        "        # print(\"tokens shape, \", tokens.shape)\n",
        "        embeddings = self.modules.discrete_embedding_layer(tokens)\n",
        "        # print(\"embeddings shape, \", embeddings.shape)\n",
        "        att_w = self.modules.attention_mlp(embeddings)\n",
        "        feats = torch.matmul(att_w.transpose(2, -1), embeddings).squeeze(-2)\n",
        "        # print(f\"feats shape, {feats.shape}\")\n",
        "        # tokens= tokens.float()\n",
        "\n",
        "        # outputs = self.hparams.mean_var_norm(tokens, lens)\n",
        "        # print(f\"after mean_var_norm, \", outputs.shape)\n",
        "        outputs, _ = self.modules.lstm(feats)\n",
        "        # print(f\"after lstm, \", outputs.shape)\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        # print(f\"after pooling, \", outputs.shape)\n",
        "        outputs = outputs.reshape(outputs.shape[0], -1)\n",
        "        # print(f\"after pooling and reshape, \", outputs.shape)\n",
        "        outputs = self.modules.linear_1(outputs)\n",
        "        # outputs = self.modules.linear_2(outputs)\n",
        "        # print(f\"after linear_1, \", outputs.shape)\n",
        "\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = torchaudio.transforms.Resample(fs, 24000)(sig)\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9O_pB7sCLRK"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttqMvMXeCU06",
        "outputId": "dffdef3f-70cb-4052-f5da-d84eba9cfd9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 809/809 [00:00<00:00, 5.47MB/s]\n",
            "model.safetensors: 100% 93.1M/93.1M [00:00<00:00, 195MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_rnn_v12/1986\n",
            "numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "Downloading https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1 to /path/to/GSC/noise/data.zip\n",
            "noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1: 569MB [00:39, 14.3MB/s]               \n",
            "Extracting /path/to/GSC/noise/data.zip to /path/to/GSC/noise\n",
            "Downloading https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1 to /path/to/GSC/rir/data.zip\n",
            "RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1: 246MB [00:20, 12.1MB/s]               \n",
            "Extracting /path/to/GSC/rir/data.zip to /path/to/GSC/rir\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "/usr/local/lib/python3.10/dist-packages/speechbrain/core.py:789: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=gradscaler_enabled)\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 1.3M\n",
            "* Total Number of Parameters: 16.2M\n",
            "* Trainable Parameters represent 8.2332% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 1154/1154 [04:10<00:00,  4.61it/s, train_loss=2.1]\n",
            "100% 138/138 [00:28<00:00,  4.80it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.10 - valid loss: 1.85, valid ErrorRate: 6.44e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-49-09+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 1154/1154 [04:01<00:00,  4.78it/s, train_loss=1.75]\n",
            "100% 138/138 [00:26<00:00,  5.12it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.75 - valid loss: 1.62, valid ErrorRate: 5.65e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-53-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-49-09+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 1154/1154 [03:59<00:00,  4.83it/s, train_loss=1.58]\n",
            "100% 138/138 [00:28<00:00,  4.84it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.58 - valid loss: 1.51, valid ErrorRate: 5.22e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-58-07+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-53-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 1154/1154 [04:03<00:00,  4.73it/s, train_loss=1.45]\n",
            "100% 138/138 [00:26<00:00,  5.17it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.45 - valid loss: 1.39, valid ErrorRate: 4.76e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-02-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+03-58-07+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 1154/1154 [04:03<00:00,  4.75it/s, train_loss=1.33]\n",
            "100% 138/138 [00:29<00:00,  4.72it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.33 - valid loss: 1.30, valid ErrorRate: 4.49e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-07-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-02-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100% 1154/1154 [04:00<00:00,  4.80it/s, train_loss=1.25]\n",
            "100% 138/138 [00:27<00:00,  4.95it/s]\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.25 - valid loss: 1.24, valid ErrorRate: 4.32e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-11-40+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-07-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100% 1154/1154 [03:58<00:00,  4.85it/s, train_loss=1.17]\n",
            "100% 138/138 [00:28<00:00,  4.88it/s]\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.17 - valid loss: 1.20, valid ErrorRate: 4.13e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-16-07+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-11-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100% 1154/1154 [03:57<00:00,  4.87it/s, train_loss=1.1]\n",
            "100% 138/138 [00:28<00:00,  4.90it/s]\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.10 - valid loss: 1.14, valid ErrorRate: 3.86e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-20-33+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-16-07+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100% 1154/1154 [03:59<00:00,  4.82it/s, train_loss=1.04]\n",
            "100% 138/138 [00:28<00:00,  4.92it/s]\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.04 - valid loss: 1.10, valid ErrorRate: 3.73e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-25-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-20-33+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100% 1154/1154 [04:02<00:00,  4.76it/s, train_loss=0.976]\n",
            "100% 138/138 [00:26<00:00,  5.22it/s]\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 9.76e-01 - valid loss: 1.07, valid ErrorRate: 3.59e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-29-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-25-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100% 1154/1154 [04:03<00:00,  4.73it/s, train_loss=0.931]\n",
            "100% 138/138 [00:28<00:00,  4.84it/s]\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 9.31e-01 - valid loss: 1.04, valid ErrorRate: 3.54e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-34-04+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-29-31+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100% 1154/1154 [03:58<00:00,  4.83it/s, train_loss=0.88]\n",
            "100% 138/138 [00:28<00:00,  4.83it/s]\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 8.80e-01 - valid loss: 1.03, valid ErrorRate: 3.39e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-38-32+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-34-04+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100% 1154/1154 [03:57<00:00,  4.86it/s, train_loss=0.832]\n",
            "100% 138/138 [00:28<00:00,  4.85it/s]\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 8.32e-01 - valid loss: 1.02, valid ErrorRate: 3.36e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-42-59+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_rnn_v12/1986/save/CKPT+2024-09-02+04-38-32+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            " 44% 512/1154 [01:49<02:16,  4.69it/s, train_loss=0.786]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_lstm_encodec.py\", line 347, in <module>\n",
            "    speaker_brain.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1607, in fit\n",
            "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1432, in _fit_train\n",
            "    loss = self.fit_batch(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1226, in fit_batch\n",
            "    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
            "  File \"/content/train_lstm_encodec.py\", line 59, in compute_forward\n",
            "    tokens, embeddings = self.modules.ssl_model.encode(wavs, lens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/huggingface_transformers/encodec.py\", line 237, in encode\n",
            "    tokens = self._encode_tokens(inputs, length)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/lobes/models/huggingface_transformers/encodec.py\", line 263, in _encode_tokens\n",
            "    result = self.model.encode(inputs, mask, bandwidth=self.bandwidth)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 642, in encode\n",
            "    encoded_frame, scale = self._encode_frame(frame, bandwidth, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 575, in _encode_frame\n",
            "    embeddings = self.encoder(input_values)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 303, in forward\n",
            "    hidden_states = layer(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 271, in forward\n",
            "    hidden_states = layer(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 158, in forward\n",
            "    hidden_states = self._pad1d(hidden_states, (self.padding_total, extra_padding), mode=self.pad_mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py\", line 144, in _pad1d\n",
            "    max_pad = max(padding_left, padding_right)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "!rm -rf /content/results/encodec_rnn_v12/1986/\n",
        "\n",
        "!python train_lstm_encodec.py hparams_lstm_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkCddZS9I6bW"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDelwEgwI6bW"
      },
      "source": [
        "**The graphs below depict that sequence based models will be an excellent choice since audio signals are technically sequential. We see that the model does not overfit on the data as well. The error rate also gradually decreases in an almost smooth curve. The final test error rate is 56% which is very good compared to Xvector.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcOPOHJI6bW"
      },
      "source": [
        "![image.png](attachment:a9e8ef2e-2f52-41e1-ac33-cac28be1e41c.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tieUnhmpKuj5"
      },
      "source": [
        "## hparams - GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEZx5TRI6bW"
      },
      "source": [
        "**What is GRU? Why use it?**\n",
        "**GRU is also a type of RNN as well. However, they are simpler in comparison to LSTMs with fewer gates. They are sequential and can be trained faster than LSTMs. They are robust in nature and can be tuned to give the best performance out of the lot. Same as LSTM, many parameters were tuned to enhance the output of GRUs. They are prone to overfitting so large amounts of data and a decent learning rate make for a good training recipe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1iaUmo8L0Me",
        "outputId": "5d144e25-3f02-467e-c8ca-3c89918a7b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_encodec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_encodec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/encodec_test_gru_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/encodec_24khz\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32 #32 #128\n",
        "lr: 0.00025 #0.0001\n",
        "lr_ssl: 0.00001\n",
        "hidden: 128 #128\n",
        "linear_dim: 9600 #19200 # #38400\n",
        "emb_size: 16\n",
        "dropout: 0.5\n",
        "\n",
        "sample_rate: 24000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <sslmodel_hub>\n",
        "    bandwidth: 12\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings: False\n",
        "    freeze: !ref <freeze>\n",
        "    renorm_embeddings: True\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "gru: !new:speechbrain.nnet.RNN.GRU\n",
        "        input_size: !ref <emb_size>\n",
        "        bidirectional: False\n",
        "        hidden_size: !ref <hidden>\n",
        "        num_layers: 4\n",
        "        dropout: !ref <dropout>\n",
        "        # re_init: True\n",
        "\n",
        "linear_1: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: !ref <linear_dim>\n",
        "        n_neurons: 1000\n",
        "\n",
        "linear_2: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: 1000\n",
        "        n_neurons: !ref <out_n_neurons>\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    gru: !ref <gru>\n",
        "    linear_1: !ref <linear_1>\n",
        "    linear_2: !ref <linear_2>\n",
        "    log_softmax: !ref <log_softmax>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <ssl_model>, !ref <gru>, !ref <linear_1>, !ref <log_softmax>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFzQ3vOAMVXY"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ2HBd2Yak8l",
        "outputId": "3c32f146-a78a-4655-a143-8bfa81203a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_encodec.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        tokens, embeddings = self.modules.ssl_model.encode(wavs, lens)\n",
        "        tokens= tokens.float()\n",
        "        outputs = self.hparams.mean_var_norm(tokens, lens)\n",
        "        outputs, _ = self.modules.gru(outputs)\n",
        "        outputs = outputs.reshape(outputs.shape[0], -1)\n",
        "        outputs = self.modules.linear_1(outputs)\n",
        "        outputs = self.modules.linear_2(outputs)\n",
        "\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = torchaudio.transforms.Resample(fs, 24000)(sig)\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-M87VD8MVT2"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiG5WrkbG-0",
        "outputId": "6cabece5-6eff-43d5-ade1-76b1e1f10e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████████| 809/809 [00:00<00:00, 140kB/s]\n",
            "model.safetensors: 100%|████████████████████| 93.1M/93.1M [00:00<00:00, 401MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_final_final_gru_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 10.0M\n",
            "* Total Number of Parameters: 24.8M\n",
            "* Trainable Parameters represent 40.1573% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.42it/s, train_loss=2.39]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:10<00:00, 13.40it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.39 - valid loss: 2.22, valid ErrorRate: 7.68e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-07-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.39it/s, train_loss=2.23]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.08it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.23 - valid loss: 2.15, valid ErrorRate: 7.51e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-08-33+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-07-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.32it/s, train_loss=2.15]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.55it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.15 - valid loss: 2.13, valid ErrorRate: 7.31e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-10-03+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-08-33+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100%|██████████████████████| 1153/1153 [01:22<00:00, 14.02it/s, train_loss=2.09]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:10<00:00, 13.74it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.09 - valid loss: 2.05, valid ErrorRate: 7.08e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-11-36+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-10-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.24it/s, train_loss=2.01]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.14it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 2.01 - valid loss: 2.03, valid ErrorRate: 6.90e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-13-08+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-11-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.41it/s, train_loss=1.94]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 13.99it/s]\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.94 - valid loss: 1.94, valid ErrorRate: 6.60e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-14-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-13-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.45it/s, train_loss=1.89]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.40it/s]\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.89 - valid loss: 1.90, valid ErrorRate: 6.51e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-16-08+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-14-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100%|██████████████████████| 1153/1153 [01:17<00:00, 14.93it/s, train_loss=1.85]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.32it/s]\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.85 - valid loss: 1.91, valid ErrorRate: 6.36e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-17-36+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-16-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100%|██████████████████████| 1153/1153 [01:17<00:00, 14.91it/s, train_loss=1.82]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.52it/s]\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.82 - valid loss: 1.85, valid ErrorRate: 6.23e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-19-03+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-17-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 15.01it/s, train_loss=1.79]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.18it/s]\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.79 - valid loss: 1.84, valid ErrorRate: 6.19e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-20-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-19-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|██████████████████████| 1153/1153 [01:18<00:00, 14.77it/s, train_loss=1.77]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.45it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00025 to 0.00023\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 2.50e-04, ssl_lr: 1.00e-05 - train loss: 1.77 - valid loss: 1.86, valid ErrorRate: 6.22e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-21-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|██████████████████████| 1153/1153 [01:17<00:00, 14.94it/s, train_loss=1.73]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.49it/s]\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 2.25e-04, ssl_lr: 9.00e-06 - train loss: 1.73 - valid loss: 1.83, valid ErrorRate: 6.02e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-23-26+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-20-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-21-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 15.02it/s, train_loss=1.71]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.33it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00023 to 0.0002\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 2.25e-04, ssl_lr: 9.00e-06 - train loss: 1.71 - valid loss: 1.81, valid ErrorRate: 6.05e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-24-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 15.15it/s, train_loss=1.68]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.44it/s]\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 2.03e-04, ssl_lr: 8.10e-06 - train loss: 1.68 - valid loss: 1.81, valid ErrorRate: 5.98e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-26-20+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-23-26+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-24-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|██████████████████████| 1153/1153 [01:17<00:00, 14.90it/s, train_loss=1.66]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.33it/s]\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 2.03e-04, ssl_lr: 8.10e-06 - train loss: 1.66 - valid loss: 1.79, valid ErrorRate: 5.92e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-27-47+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-26-20+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 14.98it/s, train_loss=1.64]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.75it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0002 to 0.00018\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 2.03e-04, ssl_lr: 8.10e-06 - train loss: 1.64 - valid loss: 1.77, valid ErrorRate: 5.94e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-29-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|██████████████████████| 1153/1153 [01:15<00:00, 15.27it/s, train_loss=1.62]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.82it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00018 to 0.00016\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-06 to 6.6e-06\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 1.82e-04, ssl_lr: 7.29e-06 - train loss: 1.62 - valid loss: 1.81, valid ErrorRate: 6.04e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-30-40+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-29-14+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 15.12it/s, train_loss=1.59]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.65it/s]\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 1.64e-04, ssl_lr: 6.56e-06 - train loss: 1.59 - valid loss: 1.80, valid ErrorRate: 5.91e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-32-06+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-30-40+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-27-47+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|██████████████████████| 1153/1153 [01:17<00:00, 14.79it/s, train_loss=1.57]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.24it/s]\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 1.64e-04, ssl_lr: 6.56e-06 - train loss: 1.57 - valid loss: 1.77, valid ErrorRate: 5.87e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-33-34+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-32-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|██████████████████████| 1153/1153 [01:16<00:00, 15.10it/s, train_loss=1.56]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:09<00:00, 14.16it/s]\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 1.64e-04, ssl_lr: 6.56e-06 - train loss: 1.56 - valid loss: 1.78, valid ErrorRate: 5.84e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-35-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-33-34+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/encodec_final_final_gru_v12/1986/save/CKPT+2024-04-26+18-35-01+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:10<00:00, 14.07it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 20 - test loss: 1.83, test ErrorRate: 5.89e-01\n"
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# !rm -rf /content/results/encodec_gru_v12/1986/\n",
        "\n",
        "!python train_encodec.py hparams_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JrUeBcyI6bc"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkx4aCRlI6bc"
      },
      "source": [
        "**In the loss vs epoch graph, towards the end GRU starts overfitting the data. Nonetheless, the performance is similar to LSTM and was trained in lesser time. The error rate continuously decreases throughout the epochs as well. the final test error rate is aroung 58%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7ogT-8I6bd"
      },
      "source": [
        "![image.png](attachment:0b4efe8a-5e61-46b2-a63e-c7af6490c064.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcwzhDdOKubq"
      },
      "source": [
        "## hparams - transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjXdkYZI6be"
      },
      "source": [
        "**What are transformers? Why use it?**\n",
        "**Transformers are another set of great models that deal with sequential data. However, they do not process it sequentially like RNNs and LSTMs. They process the entire sequence together, making them great at long term memory. Since audio data is a sequence, transformers conventioannly perform very well on them. I tried various different hyperparameters, but was not able to bring the true potential of transformers. I believe I am missing something in code. The following is my attempt at transformers for keyword spotting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "702nIYf7mPXL",
        "outputId": "09895e47-a5e0-452e-99fd-1d8b70719e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_encodec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_encodec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/encodec_transformer_3_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/encodec_24khz\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 20\n",
        "batch_size: 32\n",
        "lr: 0.0001 # 0.0001\n",
        "lr_ssl: 0.00001 # 0.00001\n",
        "linear_dim: 1200 #3750\n",
        "num_layers: 1\n",
        "nhead: 2\n",
        "d_ffn: 256\n",
        "\n",
        "sample_rate: 24000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze: True # False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <sslmodel_hub>\n",
        "    bandwidth:  12.0\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings: False\n",
        "    freeze: !ref <freeze>\n",
        "    renorm_embeddings: True\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "encoder: !new:speechbrain.lobes.models.transformer.Transformer.TransformerEncoder\n",
        "         num_layers: !ref <num_layers>\n",
        "         input_shape: (32, 75, 16)\n",
        "         nhead: !ref <nhead>\n",
        "         d_ffn: !ref <d_ffn>\n",
        "        #  ffn_type: 'regularFFN'\n",
        "        #  ffn_cnn_kernel_size_list: [3, 3]\n",
        "         d_model: 16\n",
        "         activation: !name:torch.nn.modules.activation.ReLU\n",
        "        #  kdim: None\n",
        "        #  vdim: None\n",
        "         dropout: 0.4\n",
        "         normalize_before: True\n",
        "\n",
        "\n",
        "linear: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: !ref <linear_dim>\n",
        "        n_neurons: !ref <out_n_neurons>\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    encoder: !ref <encoder>\n",
        "    linear: !ref <linear>\n",
        "    log_softmax: !ref <log_softmax>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <linear>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tehB9fNkMiGF"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AntBAkThmRXV",
        "outputId": "32f337cc-4ba7-400f-b3ee-26471d67b4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_encodec.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        tokens, embeddings = self.modules.ssl_model.encode(wavs, lens)\n",
        "        tokens = tokens.float()\n",
        "        outputs = self.hparams.mean_var_norm(tokens, lens)\n",
        "        outputs, _ = self.modules.encoder(outputs)\n",
        "        outputs = outputs.reshape(outputs.shape[0], -1)\n",
        "        outputs = self.modules.linear(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = torchaudio.transforms.Resample(fs, 24000)(sig)\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWLciffIMiCq"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEFAfDSCmTeT",
        "outputId": "f6c15dac-a5e8-4dd3-a3c9-da3585d96223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_transformer_3_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 24.1k\n",
            "* Total Number of Parameters: 14.9M\n",
            "* Trainable Parameters represent 0.1617% of the total size.\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-24-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.37it/s, train_loss=2.25]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:10<00:00, 13.66it/s]\n",
            "speechbrain.utils.train_logger - epoch: 6, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 2.25 - valid loss: 2.46, valid ErrorRate: 8.57e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-28-04+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-24-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100%|██████████████████████| 1153/1153 [01:21<00:00, 14.22it/s, train_loss=2.23]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 13.98it/s]\n",
            "speechbrain.utils.train_logger - epoch: 7, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 2.23 - valid loss: 2.47, valid ErrorRate: 8.54e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-29-35+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-28-04+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.57it/s, train_loss=2.22]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.32it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 2.22 - valid loss: 2.48, valid ErrorRate: 8.53e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-31-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-29-35+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.58it/s, train_loss=2.21]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 13.98it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 9, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 2.21 - valid loss: 2.48, valid ErrorRate: 8.57e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-32-34+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100%|███████████████████████| 1153/1153 [01:17<00:00, 14.87it/s, train_loss=2.2]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.10it/s]\n",
            "speechbrain.utils.train_logger - epoch: 10, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 2.20 - valid loss: 2.48, valid ErrorRate: 8.54e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-34-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-32-34+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100%|███████████████████████| 1153/1153 [01:18<00:00, 14.69it/s, train_loss=2.2]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.02it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-06 to 6.6e-06\n",
            "speechbrain.utils.train_logger - epoch: 11, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 2.20 - valid loss: 2.49, valid ErrorRate: 8.54e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-35-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-34-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.46it/s, train_loss=2.19]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.15it/s]\n",
            "speechbrain.utils.train_logger - epoch: 12, lr: 6.56e-05, ssl_lr: 6.56e-06 - train loss: 2.19 - valid loss: 2.49, valid ErrorRate: 8.52e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-37-00+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-31-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-35-30+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100%|██████████████████████| 1153/1153 [01:18<00:00, 14.62it/s, train_loss=2.19]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:10<00:00, 13.65it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-05 to 5.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 6.6e-06 to 5.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 13, lr: 6.56e-05, ssl_lr: 6.56e-06 - train loss: 2.19 - valid loss: 2.49, valid ErrorRate: 8.53e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-38-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.57it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:10<00:00, 13.74it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-05 to 5.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.9e-06 to 5.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 14, lr: 5.90e-05, ssl_lr: 5.90e-06 - train loss: 2.18 - valid loss: 2.50, valid ErrorRate: 8.55e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-39-59+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-38-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100%|██████████████████████| 1153/1153 [01:18<00:00, 14.69it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 13.96it/s]\n",
            "speechbrain.utils.train_logger - epoch: 15, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 2.18 - valid loss: 2.50, valid ErrorRate: 8.53e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-41-27+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-39-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.56it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.02it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-05 to 4.8e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 5.3e-06 to 4.8e-06\n",
            "speechbrain.utils.train_logger - epoch: 16, lr: 5.31e-05, ssl_lr: 5.31e-06 - train loss: 2.18 - valid loss: 2.50, valid ErrorRate: 8.53e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-42-57+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-41-27+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.35it/s, train_loss=2.18]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 14.06it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-05 to 4.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.8e-06 to 4.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 17, lr: 4.78e-05, ssl_lr: 4.78e-06 - train loss: 2.18 - valid loss: 2.51, valid ErrorRate: 8.52e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-44-27+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-42-57+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-37-00+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100%|██████████████████████| 1153/1153 [01:18<00:00, 14.68it/s, train_loss=2.17]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:09<00:00, 13.87it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-05 to 3.9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 4.3e-06 to 3.9e-06\n",
            "speechbrain.utils.train_logger - epoch: 18, lr: 4.30e-05, ssl_lr: 4.30e-06 - train loss: 2.17 - valid loss: 2.51, valid ErrorRate: 8.52e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-45-56+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100%|██████████████████████| 1153/1153 [01:20<00:00, 14.36it/s, train_loss=2.17]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:10<00:00, 13.76it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.9e-05 to 3.5e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.9e-06 to 3.5e-06\n",
            "speechbrain.utils.train_logger - epoch: 19, lr: 3.87e-05, ssl_lr: 3.87e-06 - train loss: 2.17 - valid loss: 2.51, valid ErrorRate: 8.54e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-47-27+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-45-56+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100%|██████████████████████| 1153/1153 [01:19<00:00, 14.52it/s, train_loss=2.17]\n",
            "100%|█████████████████████████████████████████| 138/138 [00:10<00:00, 13.48it/s]\n",
            "speechbrain.utils.train_logger - epoch: 20, lr: 3.49e-05, ssl_lr: 3.49e-06 - train loss: 2.17 - valid loss: 2.51, valid ErrorRate: 8.50e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-48-57+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-44-27+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-47-27+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/encodec_transformer_3_v12/1986/save/CKPT+2024-04-26+21-48-57+00\n",
            "100%|█████████████████████████████████████████| 153/153 [00:10<00:00, 14.11it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 20 - test loss: 2.50, test ErrorRate: 8.54e-01\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!rm -rf /content/results/encodec_transformer_v12/1986/\n",
        "\n",
        "!python train_encodec.py hparams_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueh5vbrmI6bh"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWyuu-aLI6bh"
      },
      "source": [
        "**As mentioned earlier, the model heavily overfits the data. This is most proabably because the Transformer model is overbearing for a small and simple dataset like the google speech command dataset. The error rate also does not see a huge dip, stationing itself at about 86%. The final test error rate is 85%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "folrjkMSI6bi"
      },
      "source": [
        "![image.png](attachment:6039770e-2fd9-42ea-a9f0-59955caaf635.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzGeGxtJI6bi"
      },
      "source": [
        "## LSTM VS GRU VS XVECTOR VS TRANSFORMER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PONlxcv_I6bi"
      },
      "source": [
        "**The below graph depicts how the loss decreases/increases throughout the epochs. Clearly, LSTM and GRU perform great versus Xvector and Transformer. They all saturate near the 20 epocch mark.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abk-YZcNI6bi"
      },
      "source": [
        "![image.png](attachment:19b45f77-112f-4b41-981c-d8d8313b0583.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8EqeHiI6bj"
      },
      "source": [
        "**Likewise, the grpah below depicts the error rates for all the models. Following the same fashion LSTM leads the race, reaching rates of 55%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-rH2_MfI6bj"
      },
      "source": [
        "![image.png](attachment:aa8c1f5f-7c9b-48be-b507-f4b24211e4fa.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HoSXjdSK8VJ"
      },
      "source": [
        "## hparams - CRDNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNOC9yBpI6bj"
      },
      "source": [
        "**I tried to apply CRDNN on the Encodec features. I could not produce desired results while training. I am making some elementary errors. The below is my attempt at CRDNN.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFO8Ifo9p_C9",
        "outputId": "7a7a8d9b-1008-4b47-dbcd-0d9c0e30eec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_encodec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_encodec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/encodec_la_crdnn_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/encodec_24khz\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001 # 0.0001\n",
        "lr_ssl: 0.00001 # 0.00001\n",
        "linear_dim: 1024 #3750\n",
        "# rnn_class: <class 'speechbrain.nnet.RNN.LiGRU'>\n",
        "\n",
        "sample_rate: 24000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze: True # False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <sslmodel_hub>\n",
        "    bandwidth:  12.0\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings: False\n",
        "    freeze: !ref <freeze>\n",
        "    # renorm_embeddings: True\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "encoder: !new:speechbrain.lobes.models.CRDNN.CRDNN\n",
        "         input_size: 16\n",
        "         activation: !name:torch.nn.modules.activation.LeakyReLU\n",
        "         rnn_class: !name:speechbrain.nnet.RNN.LSTM\n",
        "         rnn_neurons: 128\n",
        "         rnn_layers: 2\n",
        "         dropout: 0.5\n",
        "         rnn_bidirectional: False\n",
        "         input_shape: (32, 75, 16)\n",
        "         cnn_blocks: 2\n",
        "         cnn_channels: [128, 256]\n",
        "         cnn_kernelsize: (3, 3)\n",
        "         time_pooling: False\n",
        "         time_pooling_size: 2\n",
        "         freq_pooling_size: 2\n",
        "         inter_layer_pooling_size: [2, 2]\n",
        "         using_2d_pooling: False\n",
        "         rnn_re_init: False\n",
        "         dnn_blocks: 2\n",
        "         dnn_neurons: 512\n",
        "         projection_dim: -1\n",
        "         use_rnnp: False\n",
        "\n",
        "linear_1: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: !ref <linear_dim>\n",
        "        n_neurons: 256\n",
        "\n",
        "linear_2: !new:speechbrain.nnet.linear.Linear\n",
        "        input_size: 256\n",
        "        n_neurons: !ref <out_n_neurons>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    encoder: !ref <encoder>\n",
        "    linear_1: !ref <linear_1>\n",
        "    linear_2: !ref <linear_2>\n",
        "    log_softmax: !ref <log_softmax>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <linear_1>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL4qLXBRMn6c"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ZDcnjoqBSm",
        "outputId": "6965d43b-928c-49ed-d368-2dad4d80f952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_encodec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_encodec.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        # We use Encodec as a discrete feature extractor on the raw waveforms. We obtain\n",
        "        # tokens(discrete features) and embedding(continuous features)\n",
        "        tokens, embeddings = self.modules.ssl_model.encode(wavs, lens)\n",
        "        tokens = tokens.float()\n",
        "        outputs = self.hparams.mean_var_norm(tokens, lens)\n",
        "        outputs = self.modules.encoder(outputs)\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.reshape(outputs.shape[0], -1)\n",
        "        outputs = self.modules.linear_1(outputs)\n",
        "        outputs = self.modules.linear_2(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration, target_sr=24000):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        # sig = torchaudio.transforms.Resample(fs, 24000)(sig)\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwBnRaLYMn27"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcsWScShqDNw",
        "outputId": "ebd4e281-1f00-408e-993a-2552dcb4d981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████████████████████████| 809/809 [00:00<00:00, 120kB/s]\n",
            "model.safetensors: 100%|████████████████████| 93.1M/93.1M [00:00<00:00, 342MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "speechbrain.lobes.models.huggingface_transformers.huggingface - EncodecModel is frozen.\n",
            "huggingface_Encodec - Encodec is frozen.\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/encodec_la_crdnn_v12/1986\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 2.4M\n",
            "* Total Number of Parameters: 17.2M\n",
            "* Trainable Parameters represent 13.7600% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100%|██████████████████████| 1157/1157 [01:18<00:00, 14.68it/s, train_loss=2.51]\n",
            "100%|█████████████████████████████████████████| 139/139 [00:08<00:00, 16.01it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 2.51 - valid loss: 2.49, valid ErrorRate: 9.26e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/encodec_la_crdnn_v12/1986/save/CKPT+2024-04-26+20-53-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            " 85%|███████████████████▌   | 982/1157 [01:05<00:12, 14.29it/s, train_loss=2.51]^C\n",
            " 85%|███████████████████▌   | 983/1157 [01:05<00:11, 14.98it/s, train_loss=2.51]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/notebooks/train_encodec.py\", line 343, in <module>\n",
            "    speaker_brain.fit(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/speechbrain/core.py\", line 1592, in fit\n",
            "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/speechbrain/core.py\", line 1417, in _fit_train\n",
            "    loss = self.fit_batch(batch)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/speechbrain/core.py\", line 1218, in fit_batch\n",
            "    scaled_loss.backward()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 525, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# !rm -rf /content/results/encodec_crdnn_v12/1986/\n",
        "\n",
        "!python train_encodec.py hparams_encodec.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQDGFiSPjhRT"
      },
      "source": [
        "# DAC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dbqxx_0qIw"
      },
      "source": [
        "### fine-tuning and freezing the supervised feature weights yield in the same performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yQEB8Mw1Pwx"
      },
      "source": [
        "## hparams - not loading the pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM83G3c7jiAj",
        "outputId": "01b7a27b-5e9d-4136-bee8-c05a9b827346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_dac.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_dac.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/dac_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 32\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "encoder_dim: 31\n",
        "load_pretrained: False\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.discrete.dac.DAC\n",
        "    load_pretrained: !ref <load_pretrained>\n",
        "    sample_rate: None\n",
        "    model_type: \"16KHz\"\n",
        "    quantizer_dropout: False\n",
        "    # save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqus_sGb1X4I"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-hJf1uo1TK6",
        "outputId": "70134793-e766-4550-e696-0c5c8a61a27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_dac.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_dac.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # if isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "        #     # Feature extraction and normalization\n",
        "        #     feats = self.modules.compute_features(wavs)\n",
        "        #     feats = self.modules.mean_var_norm(feats, lens)\n",
        "        outputs = self.modules.ssl_model.encode(wavs.unsqueeze(1))\n",
        "        codes = outputs[1].float()\n",
        "\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(codes, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs, lens\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        # if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "        #     command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        command = command.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stage_stats[\"ErrorRate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "            \"Initializes the ssl optimizer and model optimizer\"\n",
        "            self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "                self.modules.ssl_model.parameters()\n",
        "            )\n",
        "            self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"ssl_opt\", self.ssl_optimizer\n",
        "                )\n",
        "                self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "            self.optimizers_dict = {\n",
        "                \"model_optimizer\": self.optimizer,\n",
        "                \"ssl_optimizer\": self.ssl_optimizer,\n",
        "            }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGv4VKwW1bO-"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoSRvN8WI6bv"
      },
      "source": [
        "**The other discrete feature extractor I tried was DAC. However, the results were very bad even after trying all hyperparameter tunings. Below is a glimpse of the results for every combination of encoders and linear layers. It does not reduce at all even after multiple epochs elapse. I believe I made a mistake with extracting the right features.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaZCx-bV1VTt",
        "outputId": "1ed9f510-56f0-43eb-c1e8-cc81dc04f1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
            "  if ismodule(module) and hasattr(module, '__file__'):\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/dac_v12/1986\n",
            "numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "/path/to/GSC/noise/data.zip exists. Skipping download\n",
            "/path/to/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - SpeakerBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 76.7M\n",
            "* Total Number of Parameters: 76.7M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0% 0/1154 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100% 1154/1154 [06:21<00:00,  3.03it/s, train_loss=117]\n",
            "100% 138/138 [00:41<00:00,  3.36it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.17e+02 - valid loss: 82.86, valid ErrorRate: 9.15e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+21-58-47+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 1154/1154 [06:14<00:00,  3.08it/s, train_loss=78.4]\n",
            "100% 138/138 [00:36<00:00,  3.78it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 78.36 - valid loss: 74.47, valid ErrorRate: 9.18e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-05-39+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 1154/1154 [06:13<00:00,  3.09it/s, train_loss=70.9]\n",
            "100% 138/138 [00:36<00:00,  3.80it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 70.94 - valid loss: 67.65, valid ErrorRate: 9.17e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-12-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-05-39+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 1154/1154 [06:14<00:00,  3.09it/s, train_loss=64.3]\n",
            "100% 138/138 [00:36<00:00,  3.80it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - epoch: 4, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 64.29 - valid loss: 61.44, valid ErrorRate: 9.17e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-19-26+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-12-30+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 1154/1154 [06:13<00:00,  3.09it/s, train_loss=58.3]\n",
            "100% 138/138 [00:36<00:00,  3.79it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-05 to 6.6e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 7.3e-06 to 6.6e-06\n",
            "speechbrain.utils.train_logger - epoch: 5, lr: 7.29e-05, ssl_lr: 7.29e-06 - train loss: 58.27 - valid loss: 55.60, valid ErrorRate: 9.19e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-26-22+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/dac_v12/1986/save/CKPT+2024-04-12+22-19-26+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/dac_v12/1986/save/CKPT+2024-04-12+21-58-47+00\n",
            "100% 154/154 [00:41<00:00,  3.73it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 82.45, test ErrorRate: 9.16e-01\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/results/dac_v12/1986/\n",
        "\n",
        "!python train_dac.py hparams_dac.yaml --data_folder=/path/to/GSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-3Z9tWyI6bw"
      },
      "source": [
        "# CONCLUSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qqgkD9ZI6bw"
      },
      "source": [
        "![image.png](attachment:bac796ae-3589-4e5d-8af0-a11724aa346b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzxZtWTI6bx"
      },
      "source": [
        "**For the discrete features, RNNs are the best performing. Mixed models like CRDNN can indeed give better results. Since audio signals are sequential, transformers and RNN based models are the best bet for now with discrete features. As we can see in the above graph, continuous features still outperform discrete features. This has obvious reasons as continuous features are real valued vectors that are built around the task itself. On the other hand, discrete features target categorical aspects of input data such as phenomes or words, in speech recognition. They generalize more and are compact in nature. They greatly reduce the compute, making them resource efficient. This in turn can be helpful in many ways. They are paramount to bridging the gap between audio and NLP. In the coming future, the hope is that better discrete feature extractors are born, bringing us one step closer to AGI.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}